\section{Concluding Remarks}
From a HPC and system perspective, we believe that \name represents a revolutionary transformation in the large model training landscape. While our implementation, \name-100B, enables 8x increase in model sizes, over 10x in throughput improvement, achieves super-linear speedups on modern GPU clusters, and trains the largest model in the world, it is still just a tip of the iceberg. \name in its entirety has the potential to increase the model size by yet another order of magnitude, enabling the training of trillion parameter models of the future. 

Perhaps, what we feel most optimistic about \name is that it imposes no hurdles on the data scientists. Unlike existing approaches such as MP and PP, no model refactoring is necessary, and it is as easy to use as standard DP, making \name a prime candidate for future investigations on large model training. Through open sourcing and community feedback, we plan to make \name fully accessible to the DL community to catalyze the evolution and democratization of large model training at scale.  