\section{Related Work}
\label{sec:related-work}
\subsection{Data, Model and Pipeline Parallelism}
For a model that fits in the device memory for training, data parallelism (DP) is used to scale training to multiple devices. \uline{In DP, model parameters are replicated on each device. At each step, a mini-batch is divided evenly across all the data parallel processes, such that each process executes the forward and backward propagation on a different subset of data samples, and uses averaged gradients across processes to update the model locally.}

When a model does not fit in the device memory, model parallelism (MP) \cite{DBLP:journals/corr/mesh-tensor, megatronlm} and pipeline parallelism (PP) \cite{GPipe, DBLP:journals/corr/pipedream} split the model among processes, in vertical and horizontal way respectively.

PP splits a model horizontally across layers running each partition on a different device and use micro-batching to hide the pipeline bubble \cite{GPipe,DBLP:journals/corr/pipedream}.
\uline{Model functionalities such as tied-weights and batch-normalization are difficult to implement due to horizontal splitting and micro-batching, respectively.}

Popular PP implementation such as G-pipe \cite{GPipe} partitions both model parameters and total activations but requires a batch size proportional to number of pipeline partitions to hide the pipeline bubble.
\uline{The large batch size can affect the convergence rate, while also requiring significant memory to store activations.}

A different implementation of PP in PipeDream \cite{narayanan2019pipedream} keeps multiple copies of stale parameters to hide the pipeline bubble without increasing the batch size significantly, making it less memory efficient.
\uline{Additionally, the implementation is not equivalent to the standard DL training and has implications on training convergence.}

\subsection{Non-parallelism based approach to reduce memory}

\subsubsection{Reducing Activation Memory}

Multiple efforts have focused on reducing the memory footprint of activations through compression~\cite{jain2018gist}, activation checkpointing~\cite{DBLP:journals/corr/ChenXZG16, Jain2019CheckmateBT}, or live analysis \cite{DBLP:journals/corr/abs-1801-04380}. \uline{These efforts are complimentary and can work together with \name.} \uline{In fact, activation memory reduction in \name-R works in parallel with activation checkpointing.}

\subsubsection{CPU Offload}

\cite{layer2layer, 7783721} exploit heterogeneous nature of today's compute nodes, offloading model states to CPU memory through algorithmic design or virtualized memory, respectively. Up to $50\%$ of training time can be spent on GPU-CPU-GPU transfers \cite{layer2layer}. \uline{\name differs in that it reduces the memory consumption significantly without storing the model states to CPU memory whose bandwidth is severely constrained due to PCI-E.} \uline{On rare cases, \name-R may offload just the activation checkpoints for very large models to improve performance (see Sec.~\ref{sec:p_a} for details).}

\subsubsection{Memory Efficient Optimizer}

\cite{DBLP:journals/corr/adafactor,Anil2019MemoryEfficientAO} focus on reducing memory consumption of adaptive optimization methods by maintaining coarser-grained statistics of model parameters and gradients, with potential impact on model convergence guarantees. \uline{\name is orthogonal to these efforts, and its optimizations 
do not change the model optimization method or affect model convergence, but effectively reduce memory footprint of optimizer states and gradients per device.}

\subsection{Training Optimizers }

Adaptive optimization methods~\cite{10.5555/Adagrad,DBLP:journals/corr/Adam,DBLP:journals/corr/You-LARS,DBLP:journals/corr/You-LAMB} are crucial to achieving SOTA performance and accuracy for effective model training of large models. \uline{Compared to SGD, by maintaining fine-grained first-order and second-order statistics for each model parameter and gradient at the cost of significant memory footprint.}