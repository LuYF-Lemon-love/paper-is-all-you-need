\section{Deep Dive into \name-DP}\label{sec:memoryoptimization}

Figure \ref{fig:memory-consumption} quantifies and visualizes the memory requirement with and without \name-DP. The figure shows the memory footprint after partitioning (1) optimizer state, (2) gradient and (3) parameter redundancies accumulatively.

\subsection{P$_{os}$ : Optimizer State Partitioning }\label{sec:pos}

For a DP degree of $N_d$, we group the optimizer states into $N_d$ equal partitions, such that the $i^{th}$ data parallel process only updates the optimizer states corresponding to the $i^{th}$ partition. \uline{Thus, each data parallel process only needs to store and update $\frac{1}{N_d}$ of the total optimizer states and then only update $\frac{1}{N_d}$ of the parameters.} 
\uline{We perform an all-gather across the data parallel process at the end of each training step to get the fully updated parameters across all data parallel process.}

\textbf{Memory Savings: } As shown in Figure \ref{fig:memory-consumption}, the memory consumption after optimizing state partition reduces from \uline{$4\Psi+K\Psi$ to $4\Psi + \frac{K\Psi}{N_d}$}.

\subsection{P$_g$: Gradient Partitioning}\label{sec:pg}

As each data parallel process only updates its corresponding parameter partition, it only needs the reduced gradients for the corresponding parameters. \uline{Therefore, as each gradient of each layer becomes available during the backward propagation, we only reduce them on the data parallel process responsible for updating the corresponding parameters. After the reduction we no longer need the gradients and their memory can be released.} This reduces the memory footprint required to hold the gradients from \uline{$2\Psi$ bytes to $\frac{2\Psi}{N_d}$}. 

Effectively this is a Reduce-Scatter operation, where gradients corresponding to different parameters are reduced to different process. \uline{To make this more efficient in practice, we use a bucketization strategy, where we bucketize all the gradients corresponding to a particular partition, and perform reduction on the entire bucket at once.} In our case we perform a reduction instead of an all-reduce at the partition boundaries to reduce memory footprint and overlap computation and communication.

\textbf{Memory Savings:} By removing both gradient and optimizer state redundancy, we reduce the memory footprint further down to \uline{$ 2 \Psi + \frac{14\Psi}{N_d} \approx 2 \Psi$}.

\subsection{P$_p$: Parameter Partitioning}\label{sec:pp}

Just as with the optimizer states, and the gradients, each process only stores the parameters corresponding to its partition. When the parameters outside of its partition are required for forward and backward propagation, they are received from the appropriate data parallel process through broadcast. \uline{While this may seem to incur significant communication overhead at first glance, we show that this approach only increases the total communication volume of a baseline DP system to $1.5$x, while enabling memory reduction proportional to $N_d$.}

\textbf{Memory Savings:} With parameter partitioning, we reduce the memory consumption of an $\Psi$ parameter model from \uline{$16\Psi$ to $\frac{16\Psi}{N_d}$}.  This has a profound implication: \uline{\name powers DP to fit models with arbitrary sizeâ€” as long as there are sufficient number of devices to share the model states}.

\subsection{Implication on Model Size}\label{sec:summarymemoryoptimization}

The three phases of partitioning $P_{os}$, $P_{os+g}$, and $P_{os+g+p}$ reduces the memory consumption of each data parallel process on model states by up to 4x, 8x, and $N_d$ respectively.
Table \ref{tab:memory-consumption} analyzes model-state memory consumption of a few example models under the 3 stages of \name-DP optimizations for varying DP degree.
  
\begin{table*}
  \centering
  \begin{tabular}{|c||c|c|c||c|c|c||c|c|c|}
  \hline
  \multicolumn{1}{|c}{\multirow{2}{*}{DP}}&\multicolumn{3}{||c}{\textbf{7.5B Model} (GB)} &\multicolumn{3}{||c}{\textbf{128B Model} (GB)} & \multicolumn{3}{||c|}{\textbf {1T Model} (GB)} \\
  \hhline{~---------}
  &P$_{os}$&P$_{os+g}$&P$_{os+g+p}$&P$_{os}$&P$_{os+g}$&P$_{os+g+p}$&P$_{os}$&P$_{os+g}$&P$_{os+g+p}$\\
  \hline
  1&120&120&120&2048&2048&2048&16000&16000&16000\\
  4&52.5&41.3&\textbf{30}&896&704&512&7000&5500&4000\\
  16&35.6&\textbf{21.6}&7.5&608&368&128&4750&2875&1000\\
  64&\textbf{31.4}&16.6&1.88&536&284&\textbf{32}&4187&2218&250\\
  256&30.4&15.4&0.47&518&263&8&4046&2054&62.5\\
  1024&30.1&15.1&0.12&513&257&2&4011&2013&\textbf{15.6}\\
  \hline
  \end{tabular}
\caption{Per-device memory consumption of different optimizations in \name-DP as a function of DP degree . Bold-faced text are the combinations for which the model can fit into a cluster of 32GB V100 GPUs.}
\label{tab:memory-consumption}
\end{table*}

\section{Deep Dive into \name-R}

\subsection{$P_a$: Partitioned Activation Checkpointing}
\label{sec:p_a}

As discussed in \ref{sec:mp_activation_replication}, MP by design requires a replication of the activations, resulting in redundant copies of the activations across model parallel GPUs.

\uline{\name eliminates this redundancy by partitioning the activations, and only materializes them in a replicated form one activation layer at a time, right before the activation is used in computation.}
More specifically, once the forward propagation for a layer of a model is computed, the input activations are partitioned across all the model parallel process, until it is needed again during the backprogation. At this point, \name uses an all-gather operation to re-materialize a replicated copy of the activations. We refer to this optimization as \uline{$P_a$}.

It works in conjunction with \uline{activation checkpointing} \cite{DBLP:journals/corr/ChenXZG16}, storing partitioned activation checkpoints only instead of replicated copies.

Furthermore, in the case of very large models and very limited device memory, these partitioned activation checkpoints can also be offloaded to the CPU reducing the activation memory overhead to nearly zero at an additional communication cost, which we will discuss in \ref{sec:communication}. We refer to this as \uline{$P_{a+cpu}$}.

\textbf{Memory Saving} \uline{With partitioned activation checkpointing, \name reduces the activation footprint by a factor proportional to the MP degree.} 

\subsection{C$_B$: Constant Size Buffers}\label{sec:buffer}

\name carefully selects the sizes of the temporal-data buffers to balance memory and compute efficiency.

\uline{During training, the computational efficiency of some operations can be highly dependent on the input size, with larger inputs achieving higher efficiency.} For example, a large all-reduce operation achieves much higher bandwidth than a smaller one. Hence, to get better efficiency, high performance libraries such as NVIDIA Apex or Megatron fuses all the parameters into a single buffer before applying these operations. However, the memory overhead of the fused buffers is proportional to the model size, and can become inhibiting. \uline{To address this issue, we simply use a performance-efficient constant-size fused buffer when the model becomes too large.} By doing so, the buffer size does not depend on the model size, and by keeping the buffer size large enough, we can still achieve good efficiency.

\subsection{M$_D$: Memory Defragmentation}

\uline{Memory fragmentation in model training occurs as a result of activation checkpointing and gradient computation.}

During the forward propagation with activation checkpointing, only selected activations are stored for back propagation while most activations are discarded as they can be recomputed again during the back propagation. This creates an interleaving of short lived memory (discarded activations) and long lived memory (checkpointed activation), leading to memory fragmentation. Similarly, during the backward propagation, the parameter gradients are long lived, while activation gradients and any other buffers required to compute the parameter gradients are short lived. Once again, this interleaving of short term and long term memory causes memory fragmentation.

Limited memory fragmentation is generally not an issue, when there is plenty of memory to spare, but for large model training running with limited memory, memory fragmentation leads to two issues, i) \uline{OOM due to lack of contiguous memory even when there is enough available memory}, ii) \uline{poor efficiency as a result of the memory allocator spending significant time to search for a contiguous piece of memory to satisfy a memory request.}

\uline{\name does memory defragmentation on-the-fly by pre-allocating contiguous memory chunks for activation checkpoints and gradients, and copying them over to the pre-allocated memory as they are produced.} M$_D$ not only enables \name to train larger models with larger batch sizes, but also improves efficiency when training with limited memory.