\section*{Abstract}
We develop a novel solution, \textbf{Zero Redundancy Optimizer (\name)}, to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained.
\name eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: \name has the potential to scale beyond 1 \textbf{Trillion} parameters using today's hardware.