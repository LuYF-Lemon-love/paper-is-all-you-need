\begin{comment}

In addition to parallelism based approaches to reduce the memory footprint /cite{}


\section{Background}
DL training iteratively processes data samples in steps, and each step consists of a forward propagation, followed by a backward propagation, which is followed by weight updates. To scale DL training across GPUs, each step must be parallelized due to sequential dependency across steps. Data parallelism and model parallelism, are the two most prominent ways to parallize a DL training step across multiple GPUs. 

\subsection{Data Parallel Training}

For a model that fits in GPU memory for training, data parallelism (DP) is used to scale training to multiple GPUs. In data parallelism, model parameters are replicated on each GPU process during initialization. At each step, a mini-batch is divided evenly across all the data parallel processes, such that each process executes the forward and backward propagation on a different subset of data samples.  

At the end of the backward propagation, the gradients are averaged across all processes, using an all-reduce communication collective, which creates a copy of the averaged gradients on each data parallel process. Each process then uses the averaged gradients to compute and apply the weight updates to its copy of the model parameters. 

Note that in data parallelism, the OGP states (weight parameters, average gradients and the optimizer states) are replicated across all the data parallel processes, incurring massive redundancy.
%as each process performs weight updates to its local copy of the parameters.
\subsection{Model Parallel Training}
While model parallelism (MP) may serve other purposes such as reducing the aggregate batch size, it is primarily used to reduce the memory footprint per GPU. 

Model parallelism divides a model into multiple partitions and performs the corresponding forward and backward computation across devices, i.e., the computation for each input data sample is partitioned and executed on multiple devices.  Model parallelism can work in two ways: i) via vertical partitioning \cite{GPipe, DBLP:journals/corr/pipedream}, and ii) via horizontal partitioning \cite{DBLP:journals/corr/mesh-tensor, megatronlm}. Vertical partitioning splits the model by partitioning the total layers in the DL model across multiple GPUs, and uses  pipelined micro-batches to avoid GPU stalls. Horizontal partitioning on the other hand, splits individual DL layers across multiple-GPUs by parallelizing the underlying computation such as matrix-multiplication, or the element-wise operation.

While each of these techniques has their pros and cons in terms of training throughput and batch size implications, the underlying effect in terms of memory consumption is the same. Both reduces the memory footprint per GPU by partitioning the activations, model parameters, gradients, and optimizer states across multiple GPUs. 
\end{comment}