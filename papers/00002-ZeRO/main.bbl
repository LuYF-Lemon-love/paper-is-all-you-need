\begin{thebibliography}{10}

\bibitem{megatronlm}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.

\bibitem{DBLP:journals/corr/Adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem{DBLP:journals/corr/ChenXZG16}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em CoRR}, abs/1604.06174, 2016.

\bibitem{DBLP:journals/corr/batch-scaling}
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI~Dota Team.
\newblock An empirical model of large-batch training.
\newblock {\em CoRR}, abs/1812.06162, 2018.

\bibitem{DBLP:journals/corr/mesh-tensor}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake~A. Hechtman.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock {\em CoRR}, abs/1811.02084, 2018.

\bibitem{GPipe}
Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, and Zhifeng Chen.
\newblock Gpipe: Efficient training of giant neural networks using pipeline parallelism.
\newblock {\em ArXiv}, abs/1811.06965, 2018.

\bibitem{DBLP:journals/corr/pipedream}
Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil~R. Devanur, Gregory~R. Ganger, and Phillip~B. Gibbons.
\newblock Pipedream: Fast and efficient pipeline parallel {DNN} training.
\newblock {\em CoRR}, abs/1806.03377, 2018.

\bibitem{narayanan2019pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Granger, Phil Gibbons, and Matei Zaharia.
\newblock Pipedream: Generalized pipeline parallelism for dnn training.
\newblock In {\em ACM Symposium on Operating Systems Principles (SOSP 2019)}, October 2019.

\bibitem{jain2018gist}
Animesh Jain, Amar Phanishayee, Jason Mars, Lingjia Tang, and Gennady Pekhimenko.
\newblock Gist: Efficient data encoding for deep neural network training.
\newblock In {\em International Symposium on Computer Architecture (ISCA 2018)}, 2018.

\bibitem{Jain2019CheckmateBT}
Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph~E. Gonzalez.
\newblock Checkmate: Breaking the memory wall with optimal tensor rematerialization.
\newblock {\em ArXiv}, abs/1910.02653, 2019.

\bibitem{DBLP:journals/corr/abs-1801-04380}
Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen~Leon Song, Zenglin Xu, and Tim Kraska.
\newblock Superneurons: Dynamic {GPU} memory management for training deep neural networks.
\newblock {\em CoRR}, abs/1801.04380, 2018.

\bibitem{layer2layer}
Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, and Sujeeth Bharadwaj.
\newblock Training large neural networks with constant memory using a new execution algorithm.
\newblock {\em ArXiv}, abs/2002.05645, 2020.

\bibitem{7783721}
M.~{Rhu}, N.~{Gimelshein}, J.~{Clemons}, A.~{Zulfiqar}, and S.~W. {Keckler}.
\newblock vdnn: Virtualized deep neural networks for scalable, memory-efficient neural network design.
\newblock In {\em 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, pages 1--13, 2016.

\bibitem{DBLP:journals/corr/adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock {\em CoRR}, abs/1804.04235, 2018.

\bibitem{Anil2019MemoryEfficientAO}
Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Memory-efficient adaptive optimization for large-scale learning.
\newblock {\em ArXiv}, abs/1901.11150, 2019.

\bibitem{10.5555/Adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock {\em J. Mach. Learn. Res.}, 12(null):2121â€“2159, July 2011.

\bibitem{DBLP:journals/corr/You-LARS}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling {SGD} batch size to 32k for imagenet training.
\newblock {\em CoRR}, abs/1708.03888, 2017.

\bibitem{DBLP:journals/corr/You-LAMB}
Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho{-}Jui Hsieh.
\newblock Reducing {BERT} pre-training time from 3 days to 76 minutes.
\newblock {\em CoRR}, abs/1904.00962, 2019.

\end{thebibliography}
