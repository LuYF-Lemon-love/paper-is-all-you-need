\section{Communication Analysis of \name-DP}\label{sec:communication}

What is the communication volume of \name-powered DP approach compared to a baseline DP approach?
The answer is in two parts:

i) \uline{\name-DP incurs no additional communication using $P_{os}$ and $P_g$, while enabling up to 8x memory reduction},

ii) \uline{\name-DP incurs a maximum of $1.5$x communication when using $P_p$ in addition to $P_{os}$ and $P_{g}$, while further reducing the memory footprint by $N_d$ times.}

\subsection{Data Parallel Communication Volume}

During data parallel training, gradients across all data parallel processes are averaged at the end of the backward propagation before computing the updates for the next step. The averaging is performed using an all-reduce communication collective.

For a large model size, the all-reduce communication is entirely communication bandwidth bound, and therefore, we limit our analysis to the total communication volume send to and from each data parallel process.

\uline{State-of-art implementation of all-reduce uses a two-step approach, where the first step is a reduce-scatter operation, which reduces different part of the data on different process. The next step is an all-gather operation where each process gathers the reduced data on all the process. The result of these two steps is an all-reduce.}

Both reduce-scatter and all-gather are implemented using a pipelined approach, that results in a total data movement of $\Psi$ elements (for a data with $\Psi$ elements) for each. \uline{Therefore, the standard DP incurs 2$\Psi$ data movement during each training step.}

 \begin{table*}
        \centering
        %\resizebox{\columnwidth}{!}{
            \begin{tabular}{|c|c||c|c|c|c||c|c|}
            \hline
            \multicolumn{1}{|c}{\multirow{2}{*}{MP}}&\multicolumn{1}{|c}{\multirow{2}{*}{GPUs}}  & \multicolumn{4}{||c|}{\bf Max Theoretical Model Size}&\multicolumn{2}{|c|}{\bf Measured Model Size} \\
            \hhline{~~------}
            &&Baseline&P$_{os}$&P$_{os+g}$&P$_{os+g+p}$&Baseline&\name-DP (P$_{os}$)\\
            \hline
            1&64&2B&\bf{7.6B}&14.4B&128B&1.3B&\bf{6.2B}\\
            \hline
            2&128&4B&\bf{15.2B}&28.8B&256B&2.5B&\bf{12.5B}\\
            \hline
            4&256&8B&\bf{30.4B}&57.6B&0.5T&5B&\bf{25B}\\
            \hline
            8&512&16B&\bf{60.8B}&115.2B&1T&\it{10B}&\bf{50B}\\
            \hline
            16&1024&32B&\bf{121.6B}&230.4B&\it{2T}&20B&\bf{100B}\\
            \hline
            \end{tabular}
        %}
        
     \caption{Maximum model size through memory analysis (left) and the measured model size when running with \nameos (right). The measured model size with $P_{os}$ matches the theoretical maximum, demonstrating that our memory analysis provides realistic upper bounds on model sizes.}
     \label{tab:largest-model}
 \end{table*}

\subsection{\name-DP Communication Volume}
\subsubsection{Communication Volume with $P_{os+g}$}

\uline{With gradient partitioning, each process only stores the portion of the gradients, that is required to update its corresponding parameter partition. As such, instead of an all-reduce, \name only requires a scatter-reduce operation on the gradients, incurring communication volume of $\Psi$.}

\uline{After each process updates the partition of the parameters that it is responsible for, an all-gather is performed to collect all the updated parameters from all the data parallel process. This also incurs a communication volume of $\Psi$.}

So the total communication volume per training step is $\Psi+\Psi= 2\Psi$, exactly the same as the baseline DP.

\subsubsection{Communication Volume with $P_{os+g+p}$ }

After parameter partitioning, each data parallel process only stores the parameters that it updates. Therefore, during the forward propagation it needs to receives the parameters for all the other partitions.

However, this can be pipelined to avoid the memory overhead. \uline{Before computing the forward propagation on the part of the model corresponding to a particular partition, the data parallel process responsible for that partition can broadcast the weights to all the data parallel processes. Once the forward propagation for that partition is done, the parameters can be discarded. The total communication volume is thus $\frac{\Psi \times N_d}{N_d} = \Psi$.}

\uline{In other words, we reschedule the parameter all-gather by spreading it across the entire forward propagation, and discarding the parameters once they have been used.} \uline{Note however that this all-gather needs to happen once again for the backward propagation in the reverse order.}

The total communication volume is therefore the sum of the communication volumes incurred by these all-gathers in addition to the communication volume incurred by the reduce-scatter of the gradients. \uline{The total volume is therefore $3\Psi$ which is 1.5x compared to the baseline.}

\section{Communication Analysis of \name-R}
We compare the communication volume of partitioned activation checkpointing ($P_a$) in \name-R with baseline MP, and show that $P_a$ incurs a communication volume increase that is in general less than one tenth of the baseline MP. Furthermore, we analyze the communication overhead of $P_a$ in relation to DP communication volume to identify scenarios when $P_a$ improves efficiency by allowing for a larger batch size and reducing DP communication.  We leverage such analysis to decide if and when to apply  $P_a$ as well as $P_{a+cpu}$.  

Communication volume trade-off of partitioning activation checkpoints depends on the model size, checkpointing strategy and the MP strategy.  To share concrete insights, we perform the analysis in the context of transformer based models implemented using SOTA MP approach, Megatron-LM.

In Megatron-LM with activation checkpointing, each transformer block performs two all-reduce operations of size $batch \times seq\_length \times hidden\_dim$ in the forward propagation, two all-reduce for forward re-computation and two more in the backward propagation.  The total communication per block is $12 \times seq\_length \times hidden\_dim$ since communication volume of an all-reduce is $2 \times message\_size$. 

When \name-R partitions activation checkpoints, it requires an additional all-gather operation before the forward recomputation of the back-propagation on each activation checkpoint. In general, we checkpoint the input activation for each transformer block, requiring one all-gather per transformer block. The communication overhead $P_a$ is therefore $seq\_length * hidden\_dim$, since the communication volume of an all-gather is $message\_size$. Therefore, the total communication overhead of $P_a$ is less than $10\%$ of the original communication volume for model parallelism.

When MP is used in conjunction with DP, $P_a$ can be used to reduce the data-parallel communication volume by an order of magnitude at the expense of a $10\%$ increase in model-parallel communication volume, and significantly boost efficiency when data-parallel communication is a performance bottleneck. Notice that $P_a$ reduces the activation memory consumption by the MP degree allowing for a proportional increase in batch size. For large models, MP can be as large as 16 (\#GPUs on a DGX-2 node), allowing for up to 16x increase in the batch size. The communication volume of a data-parallel training is inversely proportional to the batch size. Therefore, an order of magnitude increase in batch size due to $P_a$ could result in an order-of-magnitude decrease in data-parallel communication volume. 

Finally if $P_{a+cpu}$ is applied, partitioned activation checkpoints are offloaded to CPU, reducing the activation memory requirement to nearly zero at the expense of 2x added data movement to and from CPU memory compared to $P_a$. In extreme cases where DP communication volume is the major bottleneck due to a small batch size even with $P_a$, $P_{a+cpu}$ can improve efficiency by increasing the batch size as long as the CPU data transfer overhead is less than the DP communication volume overhead, which is generally true for small batch sizes.  

Given model and hardware characteristics, we leverage the above analysis to decide if and when to apply $P_a$ and $P_{a+cpu}$.

\begin{comment}Activation partitioning and cpu checkpointing can significantly improve efficiency by increasing the training batch size. The increase in training batch size increases computation effieciency while also reducing the data parallel communication volume. Therefore, conservatively these techniques are effective as long as the reduction in data parallel communication volume is not out weighted by the communication overhead required by these optimizations. Here, we show the exact criteria, when $P_{a+cpu}$ improves efficiency via a communication volume analysis.

If we assume that the size of the checkpointed activations is $C_A$, then activation partitioning will incur a communication volume of $C_A$, since the communication volume of all-gather is equal to the message size. Similarly, CPU checkpointing will also incur a communication volume of $C_A$, as we need to transfer $C_A$ activations from CPU to the GPU memory.

With optimal activation checkpointing, $C_A$ is approximately equal to the square root of the total model activations \cite{optimal-checkpointing}, where the total model activation is highly model dependent, which makes it difficult to identify the exact communication volume overhead of $P_{a+cpu}$. However, we can come up with an exact communication volume in the context of transformer based language models.

For transformer based models such as the Bert or GPT, we generally checkpoint activations at the transformer layer boundary. Therefore, $C_A = batch \times seq\_length \times n \times h$ for a model  with $n$ layers, and $h$ hidden dimension. Comparatively, the data parallel communication volume for this model is $2 \Psi$ as described before, where $\Psi$ is the model size, which would be approximately $12 \times n \times h^2$ for a transformer based model such as the GPT-2. 
$C_A$ can become much larger than $2 \Psi$ for very large batch sizes. However, for modest batch size, $C_A$ can be much smaller than $2 \Psi$. If we consider $B_{data}$, $B_{gpu}$ and $B_{cpu}$ as the data-parallel bandwidth, gpu-gpu bandwidth and cpu-gpu memory bandwidth, $P_a$ improves efficiency as long as the following holds:

\begin{equation}
    \frac{batch \times seq\_length \times n \times h}{B_{gpu}} \leq \frac{24 \times n \times h^2}{B_{data}}  
\end{equation}

A similar equation can be derived for $P_{a+cpu}$. The above equation holds true for training very large models where the parameters and optimizer states occupy most of the memory, and the batch size we can fit is very small. Under such scenarios, $P_{a+cpu}$ offers significant efficiency improvement by allowing us to run larger batch sizes.
\end{comment}


 
 
 
 