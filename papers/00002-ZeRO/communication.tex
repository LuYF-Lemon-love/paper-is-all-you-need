\section{Communication Analysis of \name-DP}\label{sec:communication}

What is the communication volume of \name-powered DP approach compared to a baseline DP approach?
The answer is in two parts:

i) \uline{\name-DP incurs no additional communication using $P_{os}$ and $P_g$, while enabling up to 8x memory reduction},

ii) \uline{\name-DP incurs a maximum of $1.5$x communication when using $P_p$ in addition to $P_{os}$ and $P_{g}$, while further reducing the memory footprint by $N_d$ times.}

\subsection{Data Parallel Communication Volume}

During data parallel training, gradients across all data parallel processes are averaged at the end of the backward propagation before computing the updates for the next step. The averaging is performed using an all-reduce communication collective.

For a large model size, the all-reduce communication is entirely communication bandwidth bound, and therefore, we limit our analysis to the total communication volume send to and from each data parallel process.

\uline{State-of-art implementation of all-reduce uses a two-step approach, where the first step is a reduce-scatter operation, which reduces different part of the data on different process. The next step is an all-gather operation where each process gathers the reduced data on all the process. The result of these two steps is an all-reduce.}

Both reduce-scatter and all-gather are implemented using a pipelined approach, that results in a total data movement of $\Psi$ elements (for a data with $\Psi$ elements) for each. \uline{Therefore, the standard DP incurs 2$\Psi$ data movement during each training step.}

\subsection{\name-DP Communication Volume}
\subsubsection{Communication Volume with $P_{os+g}$}

\uline{With gradient partitioning, each process only stores the portion of the gradients, that is required to update its corresponding parameter partition. As such, instead of an all-reduce, \name only requires a scatter-reduce operation on the gradients, incurring communication volume of $\Psi$.}

\uline{After each process updates the partition of the parameters that it is responsible for, an all-gather is performed to collect all the updated parameters from all the data parallel process. This also incurs a communication volume of $\Psi$.}

So the total communication volume per training step is $\Psi+\Psi= 2\Psi$, exactly the same as the baseline DP.

\subsubsection{Communication Volume with $P_{os+g+p}$ }

After parameter partitioning, each data parallel process only stores the parameters that it updates. Therefore, during the forward propagation it needs to receives the parameters for all the other partitions.

However, this can be pipelined to avoid the memory overhead. \uline{Before computing the forward propagation on the part of the model corresponding to a particular partition, the data parallel process responsible for that partition can broadcast the weights to all the data parallel processes. Once the forward propagation for that partition is done, the parameters can be discarded. The total communication volume is thus $\frac{\Psi \times N_d}{N_d} = \Psi$.}

\uline{In other words, we reschedule the parameter all-gather by spreading it across the entire forward propagation, and discarding the parameters once they have been used.} \uline{Note however that this all-gather needs to happen once again for the backward propagation in the reverse order.}

The total communication volume is therefore the sum of the communication volumes incurred by these all-gathers in addition to the communication volume incurred by the reduce-scatter of the gradients. \uline{The total volume is therefore $3\Psi$ which is 1.5x compared to the baseline.}

\section{Communication Analysis of \name-R}
We compare the communication volume of partitioned activation checkpointing ($P_a$) in \name-R with baseline MP, and show that $P_a$ incurs a communication volume increase that is in general \uline{less than one tenth} of the baseline MP.

Furthermore, we analyze the communication overhead of $P_a$ in relation to DP communication volume to identify scenarios when $P_a$ improves efficiency by \uline{allowing for a larger batch size} and reducing DP communication.

Finally if $P_{a+cpu}$ is applied, partitioned activation checkpoints are offloaded to CPU, reducing the activation memory requirement to nearly zero at the expense of 2x added data movement to and from CPU memory compared to $P_a$. \uline{In extreme cases where DP communication volume is the major bottleneck due to a small batch size even with $P_a$, $P_{a+cpu}$ can improve efficiency by increasing the batch size as long as the CPU data transfer overhead is less than the DP communication volume overhead, which is generally true for small batch sizes.}