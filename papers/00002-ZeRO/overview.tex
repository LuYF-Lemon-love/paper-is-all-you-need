
\section{\name: Insights and Overview}
\name has two sets of optimizations: i) \name-DP aimed at reducing the memory footprint of the model states, and ii) \name-R targeted towards reducing the residual memory consumption.

\subsection{Insights and Overview: \name-DP}
\name powered DP is based on three key insights:

{\it a)} DP has better scaling efficiency than MP because MP reduces the granularity of the computation while also increasing the communication overhead. \uline{Beyond a certain point, lower computational granularity reduces the efficiency per GPU, while the increased communication overhead, hiders the scalability across GPUs, especially when crossing node boundaries.} On the contrary, DP has both higher computational granularity and lower communication volume, allowing for much higher efficiency.

{\it b)} \uline{DP is memory inefficient as model states are stored redundantly across all data-parallel processes. On the contrary, MP partitions the model states to obtain memory efficiency.}

{\it c)} \uline{Both DP and MP keep all the model states needed over the entire training process, but not everything is required all the time.}  For example, parameters corresponding to each layer is only needed during the forward propagation and backward propagation of the layer.  

Based on these insights, \name-DP retains the training efficiency of DP while achieving the memory efficiency of MP. \uline{\name-DP \textbf{partitions} the model states instead of replicating them (Section \ref{sec:memoryoptimization}) and uses a dynamic communication schedule that exploits the intrinsically temporal nature of the model states while minimizing the communication volume (Section \ref{sec:communication}).} By doing so, \name-DP reduces per-device memory footprint of a model \textbf{linearly} with the increased DP degree while maintaining the communication volume close to that of the default DP, retaining the efficiency.

\subsection{Insights and Overview: \name-R}

\label{sec:mp_activation_replication}
\subsubsection{Reducing Activation Memory}

Two key insights are:

{\it a)} \uline{MP partitions the model states but often requires replication of the activation memory.} For example, if we split the parameters of a linear layer vertically and compute them in parallel across two GPUs, each GPU requires the entire activation to compute its partition

{\it b)} For models such as GPT-2 or larger, the arithmetic intensity (ratio of the amount of computation per iteration to amount of activation checkpoints per iteration) is very large ($\ge 10K$) and increases linearly with hidden dimension making it possible to hide the data-movement cost for the activation checkpoints, even when the bandwidth is low.

\uline{\name removes the memory redundancies in MP by \emph{partitioning} the activations checkpoints across GPUs, and uses allgather to reconstruct them on demand.} The activation memory footprint is reduced proportional to the MP degree. \uline{For very large models, \name can even choose to move the activation partitions to the CPU memory, while still achieving good efficiency due to large arithmetic intensity in these models.}

\subsubsection{Managing Temporary buffers}

\uline{\name-R uses constant size buffers to avoid temporary buffers from blowing up as the model size increases, while making them large enough to remain efficient.}

\subsubsection{Managing fragmented Memory}

\uline{Memory fragmentation is a result of interleaving between short lived and long lived memory objects.} During the forward propagation activation checkpoints are long lived but the activations that recomputed are short lived. Similarly, the backward computation, the activation gradients are short lived while the parameter gradients are long lived. Based on this insight, \uline{\name performs on-the-fly memory defragmentation by moving activation checkpoints and gradients to pre-allocated contiguous memory buffers.} \uline{This not only increases memory availability but also improves efficiency by reducing the time it takes for the memory allocator to find free contiguous memory.}