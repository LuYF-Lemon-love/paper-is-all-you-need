\section{Step Towards 1 Trillion Parameters}
The largest published models today are in the range of 10 billion parameters, which are already challenging to train.  Getting to a trillion parameters,  3-orders of magnitude larger, will inevitably happen, but the road will be full of hurdles, surprises and innovations. While we do not claim knowing or addressing all of them, \name addresses one of the most fundamental challenges from a system perspective: the ability to fit a model of this scale on current hardware while allowing it to train with good system scalability.

\textbf{A Leap from State-of-Art}  
The largest model that the state-of-art framework, Megatron, can train with acceptable throughput is a 16 - 20B parameter model in a DGX-2 system. Scaling further by having model parallelism across multiple DGX nodes results in significant efficiency drop due to limited internode bandwidth.  

\name vastly increase the efficiently-runnable model size.  It enables the current generation of hardware to run significantly larger models without requiring fine-grained model parallelism to go across the node boundaries.
As demonstrated in Table \ref{tab:memory-consumption}, \name, with all optimizations turned on (P$_{os+g+p}$), could fit more than 1 \emph{Trillion} parameters on 1024 GPUs using DP only.  Alternatively, when combined with model parallelism (as shown in Table \ref{tab:largest-model}), \name could fit more than 1 \emph{Trillion} parameters on 1024 GPUs with 16-way model parallelism (within each DGX2 node) and 64-way data parallelism across nodes.  Running a model with a trillion parameters efficiently is no longer impossible!

 
%Combined with any form of model parallelism, \name enables the current generation of hardware to run significantly larger model without requiring fine-grained model parallelism to go across the node boundaries.
%As demonstrated in Table \ref{tab:largest-model}, \name, with all optimizations turned on (P$_{os+g+p}$), could fit $2$ \emph{Trillion} parameters on 1024 GPUs with 16-way model parallelism (within each DGX2 node) and 64-way data parallelism across nodes.  Running a model with a trillion parameters efficiently is no longer impossible!

%It is trivial to go larger by using model parallelism across multiple nodes, but as the authors of Megatron themselves noticed, this would result in a significant efficiency drop due to poor interconnect bandwidth across boxes. More specifically, the bisection bandwidth inside a DGX-2 box is about 2.4 TB/s while across boxes, the bi-section bandwidth drops to a meager 200 GB/sec. Due to this drop in bandwidth, scaling a large model past a single node via model-parallelism is not possible. 

%\textbf{Combining \name with Model Parallelism} As discussed in Sec.\ref{sec:backgroud}, Model Parallelism is used in conjunction with Data Parallelism, to train large models that would other wise not fit in a single GPU. As such, \name can be combined with any form of model parallelism, to enables the current generation hardware to run significantly larger model with the same model parallelism degree without requiring to go across node boundaries. Table.~\ref{tab:largest-model}, shows the largest model that can be run for a given degree of model parallelism and data parallelism.  In the table the baseline columns represents the current state-of-art approach. Note that with 16-way model parallelism, in theory (without any activation and other memory overheads) we can at most run a 32B parameter model with the baseline, while \name, with its various memory optimizations turned on, can fit 121 Billion, 230 Billion and 2 Trillion using $P_{os}$, $P_{os+g}$, P$_{os+g+p}$, respectively. 

\textbf{Compute Power Gap} Training a trillion parameter model end-to-end within an acceptable time range, however, could still require significant amount of compute power, which is lacking in today's AI clusters.

%Indeed, we can fit a model with over a trillion parameters using 16-way model parallelism, and 64-way data parallelism, and train it effectively since we are neither crossing the node boundary, or increasing the inter-node data-movement significantly (as discussed in Sec. ~\ref{communication-volume}). However, we would like to caution the reader, that training such a model in a reasonable time would still require significant amount of GPU resources. 

To understand the resource requirement, we present a brief comparison with Bert-Large. Bert-Large can be trained in $67$ minutes on a $1024$ GPU DGX-2H cluster \cite{nvidia-bert-training-with-gpus}. A 1 Trillion Parameter model can easily contain $3000$x (1 trillion / 330 million) more computation than a Bert-Large model for a data sample.  Even if we assume the same sequence length and the total number of samples required to train the model, training a 1T model would take 140 days, assuming the same hardware and similar computational efficiency.  In practice, both data samples and sequence length are likely to increase with the increased model size requiring over a year to train.  It would require an exa-flop system to train a 1T parameter model in a reasonable time. But when such compute capacity becomes available, we hope \name will provide the system technology to run the 1T models efficiently. 


%\textbf{Immediate Impact} While 1T parameter model requires exa-flop supercomputers, a 100B parameter model may be trained in a few weeks on a 1024 GPU system which already exists, once again assuming same sequence length and data size, and similar computational efficiency. As such, for our first release of \name, we implement $P_{os}$, which enables us to train a 100B parameter model with a 16-way model parallelism. 

%Next we discuss this implementation and demonstrate its effectiveness.

\begin{comment}


Until such a system is available, our model 

Based on our memory consumption models described in Sec.~\ref{memory-consumption-models}, 32B parameter model is the largest possible model that we fit with 16-way model parallelism. In practise, we find can fit 20B parameter using Megatron.



Going beyond a 16B parameter model would require extending model parallelism across a single node,  
The current state-of-art framework on large model training can, which can fit a 16B parameter model in a 16-GPU system such as the DGX-2 box. The authors of Megatron themselves state that currently it is imporssible to 



and in out opinion the first major road block is the ability to fit a model of this size and train it with a reasonable at a reasonable throughput. 

a major one is the ability to fit a model of this size 


With all the aforementioned memory optimizations, the memory footprint of \name is $2\Psi$ bytes for a model with $\Psi$ parameters, given enough data parallelism. On a cluster of V100 GPUs with 32GB of memory, this allows us to fit about a 14B parameter model without any model parallelism after accounting for 2GB in constant buffers. This is beyond the size of any model that we have seen in literature. Furthermore, on a HPC cluster composed of systems like the DGX-2 box, \name allows us to fit a model with over $200B$ parameters using model parallelism such as tensor slicing or model pipelining. Training such a model would require near exa-flop scale supercomputers with tens of thousands of V100 like GPUs, requiring next generation of supercomputers. However, if one were to look into the future, where tens of thousands of DGX-2 boxes were connected by infiniband like interconnect, it is interesting to ponder, if and how we could train a model with a trillion parameters. Here, we build the final piece of technology that will enable training  such a model with good efficiency, when the massively powerful hardware required to train such a model does become ready.

Note that with 14B parameters per GPU, it is possible to trivially get to a trillion parameter model by using 72 way model parallelism across multiple DGX-2 boxes. However, crossing a single DGX-2 box boundary with model parallelism greatly reduces the available inter process communication bandwidth, from near 300 GB/s between GPUs, to a meager 12.5 GB/s across GPUs. At such communication bandwidth, model parallelism will simply not scale. Therefore it is critical that the entire model fits within a single node such as a DGX-2 box.
\end{comment}
