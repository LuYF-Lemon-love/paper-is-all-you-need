\section{Where Did All the Memory Go?}

During model training, most of the memory is consumed by \textbf{model states}, i.e., tensors comprising of pptimizer states, gradients, and parameters. Besides these model states, the rest of the memory is consumed by activations, temporary buffers and fragmented memory which we call \textbf{residual states}.

\subsection{Model States: Optimizer States, Gradients and Parameters}

Majority of the device memory is consumed by model states during training.

Consider for instance, Adam~\cite{DBLP:journals/corr/Adam}, one of the most popular optimizers for DL training. Adam requires storing two optimizer states, i) \uline{the time averaged momentum} and ii) \uline{variance of the gradients to compute the updates}. Therefore, to train a model with ADAM, there has to be enough memory to hold a copy of both the momentum and variance of the gradients. In addition, there needs to be enough memory to store the gradients and the weights themselves.

\uline{Of these three types of the parameter-related tensors, the optimizer states usually consume the most memory, specially when mixed-precision training is applied.}

\textbf{Mixed-Precision Training} During mixed-precision training, both the forward and backward propagation are performed using fp16 weights and activations. However, to effectively compute and apply the updates at the end of the backward propagation, the mixed-precision optimizer keeps an fp32 copy of the parameters as well as an fp32 copy of all the other optimizer states.

\uline{Mixed precision training of a model with $\Psi$ parameters using Adam requires enough memory to hold an $fp16$ copy of the parameters and the gradients, with memory requirements of $2\Psi$ and $2\Psi$ bytes respectively.} \uline{In addition, it needs to hold the optimizer states: an $fp32$ copy of the parameters, momentum and variance, with memory requirements of $4\Psi$, $4\Psi$, and $4\Psi$ bytes, respectively.}

Let's use $K$ to denote the memory multiplier of the optimizer states, i.e., the additional memory required to store them is $K\Psi$ bytes. \uline{Mixed-precision Adam has $K=12$.} In total, this results in $2\Psi + 2\Psi + K\Psi = 16\Psi$ bytes of memory requirement.

\subsection{Residual Memory Consumption}

\textbf{Activations} can take up a significant amount of memory \cite{DBLP:journals/corr/ChenXZG16} during training.  As a concrete example, the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of 32 requires about 60\,GB of memory\footnote{The activation memory of a transformer-based model is proportional to the number of transformer layers $\times$ hidden dimensions $\times$  sequence length $\times$ batch size.  For a GPT-2 like architecture the total activations is about  $12 \times hidden\_dim \times batch \times seq\_length \times transformer\_layers$.}. \uline{Activation checkpointing (or activation recomputation) is a common approach to reduce the activation memory by approximately the square root of the total activations at the expense of $33\%$ re-computation overhead \cite{DBLP:journals/corr/ChenXZG16}.} This would reduce the activation memory consumption of this model to about 8\,GB.

\textbf{Temporary buffers} used for storing intermediate results consumes non-trivial amount of memory for large models. \uline{Operations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput.} For example, the bandwidth of all-reduce across devices improves with large message sizes. \uline{While the gradient themselves are usually stored as fp16 tensors, the fused buffer can be an fp32 tensor depending on the operation.} When the size of the model is large, these temporary buffer sizes are non-trivial. For example, for a model with 1.5B parameters, a flattened fp32 buffer would required $6\,GB$ of memory. 

\textbf{Memory Fragmentation: }  Additionally, it is possible to run out of usable memory even when there is plenty of available memory. This can happen with memory fragmentation. \uline{A request for a memory will fail if there isn't enough contiguous memory to satisfy it, even if the total available memory is larger than requested.} We observe significant memory fragmentation when training very large models, resulting in out of memory issue with over 30\% of memory still available in some extreme cases.