\section{Where Did All the Memory Go?}
Let's take a step back to examine the memory consumption of the current training system.  For example,  a 1.5B parameter GPT-2 model requires 3GB of memory for its weights (or parameters) in 16-bit precision, yet, it cannot be trained on a single GPU with 32GB memory using Tensorflow or PyTorch.  One may wonder where all the memory goes.
During model training, most of the memory is consumed by {\it model states}, i.e., tensors comprising of pptimizer states, gradients, and parameters. Besides these model states, the rest of the memory is consumed by activations, temporary buffers and fragmented memory which we call \emph{residual states}. We look at the memory consumption from both in details.
\begin{comment}
and iii) temporary buffers. It is possible to trivially  Here we look at the memory consumed by latter two of the three.
\end{comment}

\subsection{Model States: Optimizer States, Gradients and Parameters} Majority of the device memory is consumed by model states during training.  Consider for instance, Adam~\cite{DBLP:journals/corr/Adam}, one of the most popular optimizers for DL training. Adam requires storing two optimizer states, i) the time averaged momentum and ii) variance of the gradients to compute the updates. Therefore, to train a model with ADAM, there has to be enough memory to hold a copy of both the momentum and variance of the gradients. In addition, there needs to be enough memory to store the gradients and the weights themselves. Of these three types of the parameter-related tensors, the optimizer states usually consume the most memory, specially when mixed-precision training is applied.

\textbf{Mixed-Precision Training} The state-of-the-art approach to train large models on the current generation of NVIDIA GPUs is via mixed precision (fp16/32) training~\cite{micikevicius2017mixed}, where parameters and activations are stored as fp16, enabling the use of the high throughput tensor core units~\cite{nvidia-volta-arch} on these GPUs. During mixed-precision training, both the forward and backward propagation are performed using fp16 weights and activations. However, to effectively compute and apply the updates at the end of the backward propagation, the mixed-precision optimizer keeps an fp32 copy of the parameters as well as an fp32 copy of all the other optimizer states. 

Let's take Adam as a concrete example.  Mixed precision training of a model with $\Psi$ parameters using Adam requires enough memory to hold an $fp16$ copy of the parameters and the gradients, with memory requirements of $2\Psi$ and $2\Psi$ bytes respectively.  In addition, it needs to hold the optimizer states: an $fp32$ copy of the parameters, momentum and variance, with memory requirements of $4\Psi$, $4\Psi$, and $4\Psi$ bytes, respectively.  Let's use $K$ to denote the memory multiplier of the optimizer states, i.e., the additional memory required to store them is $K\Psi$ bytes. Mixed-precision Adam has $K=12$. In total, this results in $2\Psi + 2\Psi + K\Psi = 16\Psi$ bytes of memory requirement. 
For a model such as GPT-2 with $1.5$ Billion parameters, this leads to a memory requirement of at least $24\,GB$, which is significantly higher than the meager $3\,GB$ of memory required to hold the $fp16$ parameters alone.

\subsection{Residual Memory Consumption}
\textbf{Activations} can take up a significant amount of memory \cite{DBLP:journals/corr/ChenXZG16} during training.  As a concrete example, the 1.5B parameter GPT-2 model trained with sequence length of 1K and batch size of 32 requires about 60\,GB of memory\footnote{The activation memory of a transformer-based model is proportional to the number of transformer layers $\times$ hidden dimensions $\times$  sequence length $\times$ batch size.  For a GPT-2 like architecture the total activations is about  $12 \times hidden\_dim \times batch \times seq\_length \times transformer\_layers$.}.  Activation checkpointing (or activation recomputation) is a common approach to reduce the activation memory by approximately the square root of the total activations at the expense of $33\%$ re-computation overhead \cite{DBLP:journals/corr/ChenXZG16}. This would reduce the activation memory consumption of this model to about 8\,GB.

Despite the significant reduction, the activation memory can grow quite large for bigger models even with activation checkpointing. For example, a GPT-like model with 100 billion parameters requires around 60 GB of memory for batch size 32, even when using activation checkpointing.

\begin{comment}
the amount of activation memory for a transformer based language model is proportional to the batch size, sequence length, hidden dimension and the number of layers . 
Activations take a non-trivial amount of memory even when using techniques such as activation checkpointing which significantly reduce the memory required by the activations at the expense of a $33\%$ re-computation overhead\cite{}.


For example, lets consider the 1.5B parameter GPT-2 model. Without activation checkpointing a transformer model produces $O(\frac{model\_size}{hidden\_dim}*seq\_length*batch$) activation, while with activation checkpointing it produces $O(\frac{model\_size}{hidden\_dim}*seq\_length*batch$) activations if we stored the activations only at the transformer layer boundary. For a batch size of 32, the GPT-2 models will therefore produce about 60GB of activations in fp16, while with activation checkpointing, it produces about 5 GB of activations.
\end{comment}

\textbf{Temporary buffers} used for storing intermediate results consumes non-trivial amount of memory for large models.   Operations such as gradient all-reduce, or gradient norm computation tend to fuse all the gradients into a single flattened buffer before applying the operation in an effort to improve throughput. For example, the bandwidth of all-reduce across devices improves with large message sizes. While the gradient themselves are usually stored as fp16 tensors, the fused buffer can be an fp32 tensor depending on the operation. When the size of the model is large, these temporary buffer sizes are non-trivial. For example, for a model with 1.5B parameters, a flattened fp32 buffer would required $6\,GB$ of memory. 

\textbf{Memory Fragmentation: }  So far we have discussed the actual memory consumption during training. Additionally, it is possible to run out of usable memory even when there is plenty of available memory. This can happen with memory fragmentation. A request for a memory will fail if there isn't enough contiguous memory to satisfy it, even if the total available memory is larger than requested. We observe significant memory fragmentation when training very large models, resulting in out of memory issue with over 30\% of memory still available in some extreme cases.
\begin{comment}

Training a large model with many layers can runs out of memory during training, even when there is plenty of available memory. 

We find that activation checkpointing and backward propagation can cause heavy memory fragmentation, making significant portion of the GPU memory unusable, even when there is plenty of available memory. 
The memory fragmentation is caused by activation checkpointing and backward propagation allocating long term and shot term memory in an interleaved fashion. During the forward propagation, temporary buffers are created in between the creation of each activation checkpoint, causing memory fragmentation. Similarly, in backward propagation working memory is allocated between computation of the gradients, also causing memory fragmentation. We find that for models with hundreds of layers, memory fragmentation can make over 30\% of the GPU memory unusable.
\end{comment}
