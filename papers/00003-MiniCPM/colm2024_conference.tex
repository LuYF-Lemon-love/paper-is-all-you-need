\documentclass[dvipsnames]{article} % For LaTeX2e
\usepackage{colm2024_conference}

\usepackage{microtype}
\usepackage{url}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{array}
\usepackage{listings}
\usepackage{multirow}
\usepackage{caption} 
\usepackage{longtable} 
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage[frozencache=true, finalizecache=false, cachedir=./minted-cache]{minted} 

\usepackage[colorlinks, linkcolor=RoyalBlue, anchorcolor=BrickRed, citecolor=RoyalBlue, urlcolor=RoyalBlue]{hyperref}
\usepackage{CJKutf8}
\usepackage{cleveref}
\usepackage{fontawesome}
\usepackage{array}
\usepackage{ulem}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}

%\newcommand\refsec[1]{\hyperref[sec:#1]{§\ref{sec:#1}:~\textsc{#1}}}
\newcommand{\chinese}[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}
\newcommand\refsec[1]{Section~\hyperref[sec:#1]{\ref{sec:#1}}}
\newcommand\refsecs[2]{\hyperref[sec:#1]{§\ref{sec:#1}:~\textsc{#1}}, \hyperref[sec:#2]{§\ref{sec:#2}:~\textsc{#2}}}

\usepackage[most]{tcolorbox}

\tcbset{
  aibox/.style={
    width=390pt,
    top=10pt,
    colback=white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}

% 表格内容换行
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true
}


\newcommand{\OURS}{MiniCPM\xspace}



% \title{MiniCPM: Unveiling the Potential of Small Language Models}
\title{MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}


\colmfinalcopy
% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
    XXX \thanks{The original paper is located at \url{https://arxiv.org/abs/2404.06395}. This version has been modified by LuYF-Lemon-love <\url{luyanfeng_nlp@qq.com}> for personal study.}\\
    Tsinghua University
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
In this context, we introduce MiniCPM, specifically the \uline{1.2B} and \uline{2.4B} non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. \uline{For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation.} With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which \uline{we derive the much higher compute optimal data-model ratio than Chinchilla Optimal}. MiniCPM models are available publicly~\footnote{\url{https://github.com/OpenBMB/MiniCPM}}.

\end{abstract}

\input{tex/1.introduction}
\input{tex/1.relatedwork}
\input{tex/2.architecture}
\input{tex/3.lrs}
\input{tex/4.training_strategy}
\input{tex/5.modelperformance}

\newpage
\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

\newpage

\appendix

\input{tex/80.app.Bayesian}
\input{tex/81.app.quantization}
\input{tex/85.app.minicpm-speed}
\input{tex/7.CaseStudy}

\end{document}