\vspace{-5mm}
\section{Related Work}
\vspace{-2mm}
\textbf{Small Language Models.} Notable examples within the current landscape of SLMs include the \uline{Phi series} \citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, \uline{TinyLlama}~\citep{zhang2024tinyllama}, \uline{MobileLLM}~\citep{liu2024mobilellm}, and \uline{Gemma}~\citep{Banks2024Gemma}, etc. 
A variety of methodologies have been explored to augment the efficacy of SLMs. These include \uline{the incorporation of high-quality data}~\citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, \uline{the application of structure pruning techniques}~\citep{xia2023sheared}, and \uline{the reconfiguration of model architectures}~\citep{liu2024mobilellm}, among others.

\textbf{Scalable Pre-training Strategies.} Since the discovery of scaling law~\citep{kaplan2020scaling, rae2021scaling, aghajanyan2023scaling}, scientifically and predictably~\citep{achiam2023gpt,hu2023unlock, du2024understanding} scaling up the LLMs has been pursued from diverse perspectives, especially for the pre-training stage. In terms of training stability, the Tensor Program series~\citep{yang2022tensor, yang2023tensor} is introduced to ensure optimal hyper-parameter consistency across varying model scales, a technique employed in training CerebrasGPT~\citep{dey2023cerebras}. Furthermore,~\cite{wortsman2023small} suggest leveraging smaller models to anticipate and mitigate instabilities in larger model training. From the training data standpoint, various data-centric strategies have been advocated~\citep{xie2024doremi, shi2023context, ye2024data}. In the realm of training methodologies, prior research has delved into diverse learning rate schedulers (LRS)~\citep{howard2018universal, raffel2020exploring, hundt2019sharpdarts}, with the Cosine LRS~\citep{loshchilov2016sgdr} emerging as the predominant choice in LLMs. ~\cite{kaplan2020scaling} and~\cite{hoffmann2022training} have meticulously examined the hyper-parameters of Cosine LRS, thereby laying a foundational groundwork for subsequent pre-training works. Of these, DeepSeek~\citep{bi2024deepseek} bears the closest resemblance to our proposed WSD LRS. Concerning batch size scheduling,~\cite{smith2017don} advocates for incrementing batch size as an alternative to diminishing learning rate, a strategy recently adopted by Yi-9B~\citep{young2024yi}.