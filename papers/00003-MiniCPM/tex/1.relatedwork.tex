\vspace{-5mm}
\section{Related Work}
\vspace{-2mm}
\textbf{Small Language Models.} ``Small Language Models'' (SLMs) is an evolving concept that has undergone significant transformations over time. Presently, SLMs are generally construed as models that are smaller in scale compared to the well-known LLMs, typically not exceeding 7 billion parameters. These models are distinguished by their capacity for deployment on end-user devices, such as personal computers and smartphones, even in the absence of a GPU. Notable examples within the current landscape of SLMs include the Phi series \citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, TinyLlama~\citep{zhang2024tinyllama}, MobileLLM~\citep{liu2024mobilellm}, and Gemma~\citep{Banks2024Gemma}, etc. 
A variety of methodologies have been explored to augment the efficacy of SLMs. These include the incorporation of high-quality data~\citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, the application of structure pruning techniques~\citep{xia2023sheared}, and the reconfiguration of model architectures~\citep{liu2024mobilellm}, among others. MiniCPM enhances the capabilities of SLMs through a meticulous amalgamation of hyper-parameter optimization, strategic training methodologies, architectural design, and high-quality data.

\textbf{Scalable Pre-training Strategies.} Since the discovery of scaling law~\citep{kaplan2020scaling, rae2021scaling, aghajanyan2023scaling}, scientifically and predictably~\citep{achiam2023gpt,hu2023unlock, du2024understanding} scaling up the LLMs has been pursued from diverse perspectives, especially for the pre-training stage. In terms of training stability, the Tensor Program series~\citep{yang2022tensor, yang2023tensor} is introduced to ensure optimal hyper-parameter consistency across varying model scales, a technique employed in training CerebrasGPT~\citep{dey2023cerebras}. Furthermore,~\cite{wortsman2023small} suggest leveraging smaller models to anticipate and mitigate instabilities in larger model training. From the training data standpoint, various data-centric strategies have been advocated~\citep{xie2024doremi, shi2023context, ye2024data}. In the realm of training methodologies, prior research has delved into diverse learning rate schedulers (LRS)~\citep{howard2018universal, raffel2020exploring, hundt2019sharpdarts}, with the Cosine LRS~\citep{loshchilov2016sgdr} emerging as the predominant choice in LLMs. ~\cite{kaplan2020scaling} and~\cite{hoffmann2022training} have meticulously examined the hyper-parameters of Cosine LRS, thereby laying a foundational groundwork for subsequent pre-training works. Of these, DeepSeek~\citep{bi2024deepseek} bears the closest resemblance to our proposed WSD LRS. Concerning batch size scheduling,~\cite{smith2017don} advocates for incrementing batch size as an alternative to diminishing learning rate, a strategy recently adopted by Yi-9B~\citep{young2024yi}.
