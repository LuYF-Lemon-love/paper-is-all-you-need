\section{Introduction}

Recently, a resurgence of interest has been observed in the domain of SLMs, evidenced by the advent of a series of innovative models such as the Phi series~\citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, TinyLlama~\citep{zhang2024tinyllama}, MobileLLM~\citep{liu2024mobilellm}, and Gemma~\citep{Banks2024Gemma}, among others.

In this paper, we introduce MiniCPM, a series of SLMs, which primarily builds on two models, endowed with 2.4B and 1.2B non-embedding parameters respectively, and they rank preeminently in their respective 2B and 1B scale categories. MiniCPM also exhibits comparable capabilities to those of 7B$\sim$13B language models, such as Llama2-7B~\citep{touvron2023llama}, Mistral-7B~\citep{jiang2023mistral}, Gemma-7B~\citep{Banks2024Gemma}, and Llama-13B~\citep{touvron2023llama}, etc.  Notwithstanding their small model sizes, our training methodology is meticulously designed to facilitate seamless scaling of both model scale and data horizons. This is exemplified through our model wind tunnel experiments that encompass comprehensive hyper-parameter optimization (Section~\ref{MWTE}), and the deployment of a WSD (Warmup-Stable-Decay) learning rate scheduler (Section~\ref{sec:wsdlrs}). The latter is tailored for continuous training with an un-predefined pre-training token number and makes the reusing of model intermediate checkpoints highly feasible. A detailed analysis of the training dynamics of MiniCPM is presented, suggesting that the WSD scheduler demonstrates the intriguing loss landscape of model pre-training. With the WSD scheduler, we are now also capable of studying the data-model scaling law with linear effort on the model axis and a negligible effort on the data axis, while the traditional ones need quadratic effort considering the scaling along both model and data axes. The result of the scaling law indicates a much higher data size/model size ratio compared with Chinchilla Optimal~\citep{hoffmann2022training}.

Moreover, we introduce the MiniCPM family, including MiniCPM-DPO, MiniCPM-128K, and MiniCPM-MoE. We conduct evaluations of the MiniCPM family against established benchmarks and illuminate their impressive capabilities as SLMs: (1) The foundation models surpass Mistral-7B, and LLama-13B. (2) The DPO model surpasses zephyr-7B~\citep{tunstall2023zephyr} on MTBench~\citep{zheng2024judging} (3) The 2.4B MiniCPM-128K model demonstrates performance either surpassing or matching that of models like Yarn-Mistral-7B-128K~\citep{peng2023yarn} and ChatGLM3-6B-128K~\citep{du2021glm}. (4) The MiniCPM-MoE, with 4B activated parameters, is on par with Llama2-34B~\citep{touvron2023llama}.

In summary, MiniCPM propounds a new stage in the development of small language models, exemplifying the latent potential within SLMs and advocating for a more scientific and sustainable approach toward scaling up LLMs. 











