\section{Introduction}

Recently, a resurgence of interest has been observed in the domain of SLMs, evidenced by the advent of a series of innovative models such as the \uline{Phi series}~\citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, \uline{TinyLlama}~\citep{zhang2024tinyllama}, \uline{MobileLLM}~\citep{liu2024mobilellm}, and \uline{Gemma}~\citep{Banks2024Gemma}, among others.

In this paper, we introduce MiniCPM, a series of SLMs, which primarily builds on two models, endowed with 2.4B and 1.2B non-embedding parameters respectively, and they rank preeminently in their respective 2B and 1B scale categories. MiniCPM also exhibits comparable capabilities to those of 7B$\sim$13B language models, such as \uline{Llama2-7B}~\citep{touvron2023llama}, \uline{Mistral-7B}~\citep{jiang2023mistral}, \uline{Gemma-7B}~\citep{Banks2024Gemma}, and \uline{Llama-13B}~\citep{touvron2023llama}, etc. This is exemplified through our model wind tunnel experiments that encompass comprehensive hyper-parameter optimization (Section~\ref{MWTE}), and the deployment of a WSD (Warmup-Stable-Decay) learning rate scheduler (Section~\ref{sec:wsdlrs}). \uline{The latter is tailored for continuous training with an un-predefined pre-training token number and makes the reusing of model intermediate checkpoints highly feasible.} With the WSD scheduler, we are now also capable of studying the data-model scaling law with linear effort on the model axis and a negligible effort on the data axis, while the traditional ones need quadratic effort considering the scaling along both model and data axes. \uline{The result of the scaling law indicates a much higher data size/model size ratio compared with Chinchilla Optimal}~\citep{hoffmann2022training}.