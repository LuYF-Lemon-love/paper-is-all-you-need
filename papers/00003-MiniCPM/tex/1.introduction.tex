\section{Introduction}

Following the revelation of the scaling law~\citep{kaplan2020scaling}, there has been a vigorous pursuit in the field of Large Language Models (LLMs)~\citep{hoffmann2022training, bai2023qwen, team2023gemini, chowdhery2023palm, achiam2023gpt}, encompassing models with up to an astonishing number of parameters in the trillions~\citep{fedus2022switch}. These models have emerged as a pivotal driving force in the evolution of artificial intelligence.

Nonetheless, the training of such large-scale models is both financially burdensome and operationally inefficient. On one hand, the empirical understanding of the mechanisms underpinning the training of LLMs remains elusive. Given the significant economic and environmental costs, experiments on LLMs are prohibitively expensive for most researchers and corporations. On the other hand, the deployment of these colossal models in everyday scenarios, such as on personal computers or smartphones, is either inefficient or unfeasible. Both aspects underscore the imperative to refocus efforts on comprehensively exploring smaller, yet potent, language models (SLMs). These models on the one hand provide efficient solutions to practical deployment, on the other hand, if trained with scalable strategies, they can potentially guide the development of future larger models.

Recently, a resurgence of interest has been observed in the domain of SLMs, evidenced by the advent of a series of innovative models such as the Phi series~\citep{gunasekar2023textbooks, li2023textbooks, Javaheripi2023Phi2}, TinyLlama~\citep{zhang2024tinyllama}, MobileLLM~\citep{liu2024mobilellm}, and Gemma~\citep{Banks2024Gemma}, among others. While these models have significantly contributed to the expansion and diversification of the SLM landscape, there remain two pivotal areas where these models have yet to fully satisfy prevailing interests: (1) the development of comprehensive abilities akin to those exhibited by LLMs; and (2) the formulation of transparent and scalable training methodologies that could further propel the evolution of both SLMs and LLMs.

In this paper, we introduce MiniCPM, a series of SLMs, which primarily builds on two models, endowed with 2.4B and 1.2B non-embedding parameters respectively, and they rank preeminently in their respective 2B and 1B scale categories. MiniCPM also exhibits comparable capabilities to those of 7B$\sim$13B language models, such as Llama2-7B~\citep{touvron2023llama}, Mistral-7B~\citep{jiang2023mistral}, Gemma-7B~\citep{Banks2024Gemma}, and Llama-13B~\citep{touvron2023llama}, etc.  Notwithstanding their small model sizes, our training methodology is meticulously designed to facilitate seamless scaling of both model scale and data horizons. This is exemplified through our model wind tunnel experiments that encompass comprehensive hyper-parameter optimization (Section~\ref{MWTE}), and the deployment of a WSD (Warmup-Stable-Decay) learning rate scheduler (Section~\ref{sec:wsdlrs}). The latter is tailored for continuous training with an un-predefined pre-training token number and makes the reusing of model intermediate checkpoints highly feasible. A detailed analysis of the training dynamics of MiniCPM is presented, suggesting that the WSD scheduler demonstrates the intriguing loss landscape of model pre-training. With the WSD scheduler, we are now also capable of studying the data-model scaling law with linear effort on the model axis and a negligible effort on the data axis, while the traditional ones need quadratic effort considering the scaling along both model and data axes. The result of the scaling law indicates a much higher data size/model size ratio compared with Chinchilla Optimal~\citep{hoffmann2022training}.

Moreover, we introduce the MiniCPM family, including MiniCPM-DPO, MiniCPM-128K, and MiniCPM-MoE. We conduct evaluations of the MiniCPM family against established benchmarks and illuminate their impressive capabilities as SLMs: (1) The foundation models surpass Mistral-7B, and LLama-13B. (2) The DPO model surpasses zephyr-7B~\citep{tunstall2023zephyr} on MTBench~\citep{zheng2024judging} (3) The 2.4B MiniCPM-128K model demonstrates performance either surpassing or matching that of models like Yarn-Mistral-7B-128K~\citep{peng2023yarn} and ChatGLM3-6B-128K~\citep{du2021glm}. (4) The MiniCPM-MoE, with 4B activated parameters, is on par with Llama2-34B~\citep{touvron2023llama}.

In summary, MiniCPM propounds a new stage in the development of small language models, exemplifying the latent potential within SLMs and advocating for a more scientific and sustainable approach toward scaling up LLMs. 











