\section{WSD Learning Rate Scheduler}
\label{sec:wsdlrs}
\subsection{Analysing Cosine LRS}

The current commonly used learning rate strategy is the Cosine LRS~\citep{kaplan2020scaling, hoffmann2022training, rae2021scaling, touvron2023llama, bai2023qwen, almazrouei2023falcon}, which gradually decreases the learning rate following a cosine curve after it reaches its maximum after the warmup stage. 

A key hyper-parameter in the Cosine LRS is the step $T$ at which Cosine decreases to the minimum for the first time. \uline{Typically, $T$ is set to the total training step $S$ for training with a predefined training step.} Generally, it is believed that the learning rate should be high to enable sufficient exploration. We try $Cosine(T)$ and $CosineLoop(T)$ LRS, following the formula shown in Appendix~\ref{app:lrsequ}. The result can be seen in Figure~\ref{fig:cosine_lr}. We can see that when the training step is $S=20N, 40N, 60N, 80N$, the lowest loss is always achieved by the $Cosine(T)$ where $T = S$. Both $T<S$ and $T>S$ are not optimal. 

We hypothesize that the Cosine LR performs exceptionally well when $T = S$ because of the following two reasons: 

(1) Cosine LRS with $T = S$ has a longer duration of \textit{high learning rate} training compared to $T < S$ and other LRS such as Linear LRS. \uline{This high learning rate might help the model find a better global optimum.}

(2) \uline{Cosine LRS with $T = S$ has a more thorough learning rate decay phase compared to Cosine LRS with $T > S$ and Constant LRS.} This learning rate decay may involve unique training dynamics that enable the model to find a better local optimum.

\subsection{WSD LRS}

\uline{In light of the above perspective, we propose to explicitly divide the training stage into the high learning rate stage and learning rate decay stage.} We name it as the Warmup-Stable-Decay (WSD) LRS.  Especially, the WSD LRS contains three stages: \uline{the warmup stage} (whose end step is denoted by $W$), \uline{the stable training stage} (whose end step is denoted by $T$), and \uline{the remaining decay stage}. The function form of WSD is:

\vspace{-2mm}
\begin{equation}
    WSD(T; s) = \begin{cases}
       & \frac{s}{W} \eta, \quad s<W\\
       & \eta, \quad W < s < T \\
       & f(s-T)\eta,\quad T < s < S\\
    \end{cases}
\end{equation}

where $0< f(s-T)\leq 1$ is a decreasing function about $s$, $\eta$ is the maximum learning rate.

\subsection{Experiments}
\label{sec:wsd_experiments_continoustrain}

\textbf{Loss Decreases Dramatically in Decay Stage. } As shown in Figure~\ref{fig:wsd_diff_dcay}, in the decay stage, as the learning rate begins to decrease, the loss experiences a significant rapid decline and quickly decreases to be equal to or lower than the Cosine LRS at step $T=S$. \uline{At the same time, we can reuse the model before decay and continue training with the previous high learning rate.} \uline{After more steps of training $S'$, we can also perform annealing to achieve the same loss as the Cosine LRS at $Cosine(S')$.} This verifies our assumption that the training stage can be explicitly split into the stable training and decay stages.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1.05\linewidth]{Fig/WSD_diff_dcay.pdf}
        \caption{ Model training loss has a sudden decrease in the decay stage of WSD LRS. }
        \label{fig:wsd_diff_dcay}
        \vspace{0.47cm}
    \end{minipage}
    \hfill % This adds a little space between the two figures
    \begin{minipage}{0.48\linewidth}
        \centering
    \includegraphics[width=1.05\linewidth]{Fig/continuous_train.pdf}
    \caption{Continous training a 0.036B model can match the performance of 0.17B model with an acceptable increase in training compute.}
    \label{fig:continuoustrain}
    \end{minipage}
\end{figure}

\textbf{10\% Steps are Enough.} \uline{From the two-stage training perspective, shortening the decay stage will greatly benefit the fast test of different model checkpoints of stable training.} Also shown in Figure~\ref{fig:wsd_diff_dcay}, among all three stable training checkpoints in 40N, 60N, and 80N training data, having a decay of 10\% of the total tokens is sufficient to achieve the best results, while a decay of 2.5\% of total tokens falls short. \uline{Therefore, in the subsequent training experiments, we use a decay of about 10\% to ensure full convergence.}

\textbf{Effective Data Scaling with WSD LRS.} With WSD LRS, we can continuously train the LM to extreme convergence. In Figure~\ref{fig:continuoustrain}, the green lines represent 0.036B models trained with different stable training tokens. Despite the last point of the 0.036B series being trained with many more tokens than Chinchilla Optimal~\citep{hoffmann2022training}, it still has space for performance improvement. 

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}

By optimal performance, we mean the loss of training token $D$ is achieved by ${WSD}(D, 0.1D)$. With a series of $D$, the losses will form the optimal loss envelope. Due to uncertainty about the function form of the loss envelope, we try two fitting formulas: (1) exponential: $L(C) = \alpha e^{-\beta C} + L_0$ and (2) power-law: $L(C) = \beta C^{-\alpha} + L_0$. We find that the power-law form fits better (similar to the Cosine LRS~\citep{kaplan2020scaling}). In Figure~\ref{fig:continuoustrain}, the fitted curve is shown in \textcolor{darkgreen}{green} dotted lines.

To intuitively estimate and comprehend the effect of continuous training such a fixed-sized model, we also trained a 0.17B model with $WSD(40N, 4N)$, which is shown in \textcolor{pink}{pink} in Figure~\ref{fig:continuoustrain}. \uline{We can see that a 0.036B model can match the performance of a 0.17B model with an acceptable increase ($\sim 4$ times) in training compute while saving a lot of inference computation}~\citep{sardana2023beyond} (saving $\sim 5$ times per inference call), \uline{indicating a better inference-compute-optimal setting}~\citep{sardana2023beyond}.

\subsection{Analysis of the Decay Stage}

We calculate the maximum weight element update $max_{ij} (W_{ij}^{(t+1)} - W_{ij}^{(t)})$ across all weight matrices in the MiniCPM-2.4B (Introduced in Section~\ref{sec:model}). As depicted in Figure~\ref{fig:appmaxdiff}, the updates exhibit a robust correlation with the learning rate's magnitude. This observation may not be trivial: \uline{the model checkpoints experience significant updates preceding the learning rate's decay, yet the loss exhibits minimal reduction.} \uline{Conversely, during the decay stage, despite less pronounced weight alterations, there is an accelerated decrease in loss.}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{Fig/gate_proj_projection_vs_rank_25.pdf}
    \end{minipage}
    \hfill % This adds a little space between the two figures
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{Fig/q_proj_projection_vs_rank_25.pdf}
    \end{minipage}
     \caption{Max Difference of Checkpoints.}
        \label{fig:appmaxdiff}
\end{figure}

Further examination of the gradient data is undertaken by training a 0.2B model, meticulously recording every step gradient information, and evaluating the differences between consecutive steps, thereby providing an approximation of second-order gradient information.

We treat the gradient at step $t$ as a flattened vector $\mathbf{g}^{(t)}$, and the parameter (also flattened as a vector $\mathbf{x}^{(t)}$ ) update between step $t$ and $t+1$ is $\mathbf{v}^{(t)} = \mathbf{x}^{(t+1)} - \mathbf{x}^{(t)}$. The gradient norm take the $L2$ norm of the gradient $\Vert\mathbf{g}^{(t)} \Vert$, gradient inner product is $\mathbf{g}^{(t+1)} \cdot \mathbf{g}^{(t)}$, the cosine of the gradient's angle is given by $\frac{\mathbf{g}^{(t+1)} \cdot \mathbf{g}^{(t)}}  {\Vert\mathbf{g}^{(t+1)} \Vert \Vert\mathbf{g}^{(t)} \Vert}$.

Imaging the optimization process as a trajectory over a high-dimension manifold, first order directional derivative along the trajectory is computed as $D_1 = \frac{\mathbf{g}^{(t+1)} \cdot \mathbf{v}^{(t)}}{\|\mathbf{v}^{(t)}\|}
$, and the second order directional derivative is $D_2 = \frac{(\mathbf{g}^{(t+1)} - \mathbf{g}^{(t)}) \cdot \mathbf{v}^{(t)}}{\|\mathbf{v}^{(t)}\|^2}$. $D_1, D_2$ enables an approximate estimation of the loss curvature on the trajectory, $K = \frac{|D_2|}{(1 + D_1^2)^{\frac{3}{2}}}$. The results of these statistics over time are shown in Figure~\ref{fig:grad}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{Fig/grad.pdf}
    \caption{Gradient statistics over the training of a 0.2B model using WSD LRS. The exponential decay stage begins at 8000 steps.}
    \label{fig:grad}
\end{figure}

We can see that the gradient norm diminishes during the decay phase, and upon commencement of this stage, the cosine between gradients predominantly assumes positive values, suggesting that \uline{in the decay phase, model parameters undergo consistent changes across steps.} The curvature of the loss function also increases by a magnitude, indicating the proximity to a local optimum.

\subsection{Measuring the Scaling Law with WSD LRS}
\label{scalinglawwsdlrs}

\uline{Since the WSD scheduler has the advantage of arriving at the optimal loss of Cosine LRS after decaying from stable stage checkpoints of any step, we are now able to precisely measure the optimal scaling properties without re-training the models from scratch to different amounts of tokens, thus making the scaling law measurement much more efficient along the data axis.}

We measure the scaling law along the data and model axes by training SLMs of 6 sizes ranging from 0.04B to 2B, each with 6 decayed models starting from the checkpoint of $10N$ to $60N$ data during the stable training stage. The final loss of each pair of data size and model size is shown in the blue lines in Figure~\ref{fig:individual_task_datascalinglaw}. 

Then we fit the losses with model size $N$ and data size $D$ following ~\cite{hoffmann2022training} using scipy \uline{\texttt{curvefit}} function:

\begin{equation}
    L(N, D) = C_NN^{-\alpha} + C_DD^{-\beta} + L_0
\label{equ:scalinglaw}
\end{equation}

The fitted curve along the data axis for each dataset and each checkpoint are shown in orange lines in Figure~\ref{fig:individual_task_datascalinglaw}.
Then we have the optimal model size $N_{opt}$, dataset size $D_{opt}$, given a fixed amount of compute $C=6ND$~\citep{rae2021scaling} as: 

\begin{equation}
    \frac{N_{opt}}{D_{opt}} = K^2\left(\frac{C}{6}\right)^{\eta},
\label{equ:computeoptimal}
\end{equation}

where $K = (\frac{\alpha C_N}{\beta C_D})^{\frac{1}{\alpha + \beta}} $, and $\eta=\frac{\beta-\alpha}{\alpha+\beta}$. From Equation~\ref{equ:computeoptimal}, when $\alpha = \beta$, $N_{opt}/D_{opt}$ is a constant, supporting ~\cite{hoffmann2022training}'s claim, and when $\alpha < \beta$, we should emphasize more on parameter scaling~\citep{kaplan2020scaling}, and vise versa. 

In our experiments, the fitted relationship between loss and $N, D$ is shown in the contour plot of equal loss in Figure~\ref{fig:wsd_optimalscalinglaw}. We can see that in all evaluation corpora, we have $\beta < \alpha$. More specifically, on average, we have $\alpha=0.29, \beta=0.23, K^2 = 0.01, \eta=-0.10$. Since $\alpha$ is slightly larger than $\beta$, this result shows that as the computation scale, \uline{we should slightly emphasize more on data scaling than model scaling}, which aligns with ~\cite{hoffmann2022training}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\textwidth]{Fig/wsd_optimal_scaling_law.pdf}
    \caption{The fit result of the scaling experiment with WSD Scheduler. The black dots in a horizontal line denote the decayed checkpoints in different compute within the same model size.}
    \label{fig:wsd_optimalscalinglaw}
\end{figure}

A larger data-to-model ratio means that we can absorb more data into a smaller model than we previously thought, which is more efficient for inference and deployment.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.92\textwidth]{Fig/lossvscompute_fitted_and_real.pdf}
    \caption{The result of scaling experiments with WSD Scheduler (above) and the fitted scaling curve (below). The x-axis is the computation Flops $C =6ND$, each color of the line represents the same model with different computation Flops. We can see that smaller models are better than larger models when the Flops are small and worse when the Flops are large. Thus models of different sizes will intersect with each other in the plot around the compute optimal regime.}
    \label{fig:wsd_optimalscalinglaw}
\end{figure}