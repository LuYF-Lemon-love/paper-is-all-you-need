
\section{MiniCPM-V}
We also train a multimodal version of MiniCPM, namely MiniCPM-V. The details of MiniCPM-V will be introduced in a separate paper. Here, we only list a snapshot of the performance in Table~\ref{app:tabmultimodal}.
\begin{table}[]
\centering
\scalebox{0.8}{
\begin{tabular}{lcccccccc}
    \toprule
    \textbf{Model} & \textbf{Size} & \textbf{Tokens$^V$} & \textbf{MME} & \textbf{MMB$^{en}$}  & \textbf{MMB$^{zh}$} & \textbf{MMMU} & \textbf{CMMMU }\\
    \midrule
    LLaVA-Phi    & 3B   & 576  & 1335 & 59.8 & -    & -    & - \\
    MobileVLM    & 3B   & 144  & 1289 & 59.6 & -    & -    & - \\
    Imp-v1       & 3B   & 576  & 1434 & 66.5 & -    & -    & - \\
    Qwen-VL-Chat & 9.6B & 256  & 1487 & 60.6 & 56.7 & 35.9 & 30.7 \\
    CogVLM       & 17.4B& 1225 & 1438 & 63.7 & 53.8 & 32.1 & -    \\
    \midrule
   \textbf{MiniCPM-V}  & 3B   & 64   & 1452 & 67.9 & 65.3 & 37.2 & 32.1 \\
    \bottomrule
    \end{tabular}
}
\caption{Comparison with SOTA models on MLLM benchmarks.}
\label{app:tabmultimodal}
\end{table}

