
\subsection{MiniCPM-128K}
Tasks involving lengthy contexts depend on the implicit information within these contexts, circumventing the need for the extensive knowledge often absent in SLMs. In this section, we expand the context length of MiniCPM-2.4B from 4,096 to 128,000 tokens, illustrating the capability of SLM to effectively process long contexts.


\textbf{Initialization.}
For the initialization, we disable sharing embeddings between input and output, primarily to accommodate vocabulary parallelism essential for training with long context. The LM head is initialized from the input embedding.

\textbf{Training.}
Similar to MiniCPM, MiniCPM-2.4B-128K utilizes the WSD as its learning rate scheduler and reuses the last checkpoint of the stable training stage of MiniCPM-2.4B. Concerning training data, we categorize the dataset distribution detailed in Section~\ref{sec:appdatadistrbution} into ``short data'' and ``long data''. We classify books, wikis, and papers as ``long data'', and the other as the ``short data''. The training comprises 44\% long data and 56\% short data for continued training. For the extension of long contexts, we apply Adjusted Base Frequency (ABF)~\citep{xiong2023effective} in the 4K to 32k range and employ NTK-Aware RoPE Scaling~\citep{bloc97_2023_ntk} and curriculum learning from 32K to 128K. Both two stages involve future training. Furthermore, as indicated in Yi Tech Report~\citep{young2024yi} and Zebra~\citep{song2023zebra}, we use of synthetic long QA data that significantly enhances model performance in context-aware tasks.

\textbf{Evaluation.} We evaluate MiniCPM-2.4B-128K in $\infty$Bench~\citep{zhang2024infty}, a pioneering benchmark for long context evaluations. The tasks in $\infty$Bench~\citep{zhang2024infty} extend beyond typical retrieval tasks and challenge the model with long context reasoning. We can see in Table~\ref{tab:longcontext}, we achieve comparable results in Mistral-7B-Instruct-v0.2 (ABF1000w) and outperform ChatGLM3-6B-128K despite being 2.5 times smaller. 

