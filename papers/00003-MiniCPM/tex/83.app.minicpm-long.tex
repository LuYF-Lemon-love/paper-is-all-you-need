\subsection{MiniCPM-128K}

\textbf{Initialization.}
\uline{For the initialization, we disable sharing embeddings between input and output, primarily to accommodate vocabulary parallelism essential for training with long context.} \uline{The LM head is initialized from the input embedding.}

\textbf{Training.}
Similar to MiniCPM, MiniCPM-2.4B-128K utilizes the WSD as its learning rate scheduler and reuses the last checkpoint of the stable training stage of MiniCPM-2.4B. Concerning training data, we categorize the dataset distribution detailed in Section~\ref{sec:appdatadistrbution} into ``short data'' and ``long data''. \uline{We classify books, wikis, and papers as ``long data'', and the other as the ``short data''.} The training comprises 44\% long data and 56\% short data for continued training. \textbf{For the extension of long contexts, we apply Adjusted Base Frequency (ABF)~\citep{xiong2023effective} in the 4K to 32k range and employ NTK-Aware RoPE Scaling~\citep{bloc97_2023_ntk} and curriculum learning from 32K to 128K.} Both two stages involve future training. Furthermore, as indicated in Yi Tech Report~\citep{young2024yi} and Zebra~\citep{song2023zebra}, \uline{we use of synthetic long QA data that significantly enhances model performance in context-aware tasks.}

\textbf{Evaluation.} We evaluate MiniCPM-2.4B-128K in $\infty$Bench~\citep{zhang2024infty}, a pioneering benchmark for long context evaluations. \textbf{The tasks in $\infty$Bench~\citep{zhang2024infty} extend beyond typical retrieval tasks and challenge the model with long context reasoning.} We can see in Table~\ref{tab:longcontext}, we achieve comparable results in Mistral-7B-Instruct-v0.2 (ABF1000w) and outperform ChatGLM3-6B-128K despite being 2.5 times smaller.