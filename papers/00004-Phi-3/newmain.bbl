\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{NWD{\etalchar{+}}20}

\bibitem[AI]{llama3}
Meta AI.
\newblock Introducing meta llama 3: The most capable openly available llm to date.

\bibitem[Ant24]{anthropic2024claude}
AI~Anthropic.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock {\em Claude-3 Model Card}, 2024.

\bibitem[AON{\etalchar{+}}21]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
\newblock Program synthesis with large language models.
\newblock {\em arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[BBY{\etalchar{+}}23]{bai2023qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.

\bibitem[BJN{\etalchar{+}}22]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.

\bibitem[BSA{\etalchar{+}}24]{bianchi2024safetytuned}
Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou.
\newblock Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions, 2024.

\bibitem[BZGC19]{bisk2019piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock {\em arXiv preprint arXiv:1911.11641}, 2019.

\bibitem[CCE{\etalchar{+}}18]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.

\bibitem[CKB{\etalchar{+}}21]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[CLC{\etalchar{+}}19]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock In {\em Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 2924--2936, 2019.

\bibitem[CTJ{\etalchar{+}}21]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[DFE{\etalchar{+}}22]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock {\em Advances in Neural Information Processing Systems}, 35:16344--16359, 2022.

\bibitem[DZZ{\etalchar{+}}24a]{ding2024longrope}
Yiran Ding, Li~Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.
\newblock Longrope: Extending llm context window beyond 2 million tokens, 2024.

\bibitem[DZZ{\etalchar{+}}24b]{dong2024internlm}
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et~al.
\newblock Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd.
\newblock {\em arXiv preprint arXiv:2404.06512}, 2024.

\bibitem[GZA{\etalchar{+}}23]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio César~Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Gustavo de~Rosa Piero~Kauffmann, Olli Saarikivia, Adil Salim, Shital Shah, Harkirat~Singh Behl, Xin Wang, S\'ebastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need.
\newblock {\em arXiv preprint arXiv:2306.11644}, 2023.

\bibitem[HBK{\etalchar{+}}21]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the {MATH} dataset, 2021.

\bibitem[HBM{\etalchar{+}}22]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Eliza~Rutherford Trevor~Cai, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[JBA{\etalchar{+}}23]{javaheripi2023phi}
Mojan Javaheripi, S{\'e}bastien Bubeck, Marah Abdin, Jyoti Aneja, Caio César Teodoro~Mendes, Weizhu Chen, Allie Del~Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Piero Kauffmann, Yin~Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo~de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh~Behl, Adam Taumann~Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi~Zhang.
\newblock Phi-2: The surprising power of small language models.
\newblock {\em Microsoft Research Blog}, 2023.

\bibitem[JCWZ17]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017.

\bibitem[JLD{\etalchar{+}}23]{ji2023beavertails}
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce~Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset, 2023.

\bibitem[JPO{\etalchar{+}}20]{jin2020disease}
Di~Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.
\newblock What disease does this patient have? a large-scale open domain question answering dataset from medical exams, 2020.

\bibitem[JSM{\etalchar{+}}23]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem[JSR{\etalchar{+}}24]{jiang2024mixtral}
Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mixtral of experts, 2024.

\bibitem[KLZ{\etalchar{+}}23]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In {\em Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[KMH{\etalchar{+}}20]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[KSK{\etalchar{+}}16]{kembhavi2016diagram}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images, 2016.

\bibitem[LBE{\etalchar{+}}23]{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya Gunasekar, and Yin~Tat Lee.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock {\em arXiv preprint arXiv:2309.05463}, 2023.

\bibitem[LBX{\etalchar{+}}24]{lu2024mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024.

\bibitem[LDZ{\etalchar{+}}23]{li2023evaluating}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models, 2023.

\bibitem[LDZ{\etalchar{+}}24]{liu2024mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin.
\newblock Mmbench: Is your multi-modal model an all-around player?, 2024.

\bibitem[LGJ{\etalchar{+}}21]{lu2021intergps}
Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.
\newblock Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, 2021.

\bibitem[LHE22]{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods, 2022.

\bibitem[LLL{\etalchar{+}}24]{liu2024llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

\bibitem[LLLL23]{liu2023improved}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem[LLY{\etalchar{+}}24]{li2024red}
Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi~Liu.
\newblock Red teaming visual language models.
\newblock {\em arXiv preprint arXiv:2401.12915}, 2024.

\bibitem[LMX{\etalchar{+}}22]{lu2022learn}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock In {\em The 36th Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[LST{\etalchar{+}}24]{laurenccon2024obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et~al.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text documents.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[MCKS18]{mihaylov2018suit}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.

\bibitem[MGF{\etalchar{+}}24]{mckinzie2024mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang.
\newblock Mm1: Methods, analysis \& insights from multimodal llm pre-training.
\newblock {\em arXiv preprint arXiv:2403.09611}, 2024.

\bibitem[MHJ{\etalchar{+}}23]{magooda2023framework}
Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, and Mei Chen.
\newblock A framework for automated measurement of responsible ai harms in generative ai applications, 2023.

\bibitem[MLT{\etalchar{+}}22]{masry-etal-2022-chartqa}
Ahmed Masry, Do~Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock {C}hart{QA}: A benchmark for question answering about charts with visual and logical reasoning.
\newblock In {\em Findings of the Association for Computational Linguistics: ACL 2022}, pages 2263--2279, Dublin, Ireland, May 2022. Association for Computational Linguistics.

\bibitem[MRB{\etalchar{+}}23]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models.
\newblock {\em arXiv preprint arXiv:2305.16264}, 2023.

\bibitem[NWD{\etalchar{+}}20]{nie2020adversarial}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding, 2020.

\bibitem[Ope23]{gpt4v}
OpenAI.
\newblock Gpt-4v(ision) system card, 2023.
\newblock \url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}.

\bibitem[RHS{\etalchar{+}}23]{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark, 2023.

\bibitem[RKH{\etalchar{+}}21]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[RWC{\etalchar{+}}19]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem[SLBBC19]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em arXiv preprint arXiv:1907.10641}, 2019.

\bibitem[SNS{\etalchar{+}}19]{singh2019vqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read, 2019.

\bibitem[SRR{\etalchar{+}}22]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[SSS{\etalchar{+}}22]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V. Le, Ed~H. Chi, Denny Zhou, and Jason Wei.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.

\bibitem[TAB{\etalchar{+}}23]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[THLB19]{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge, 2019.

\bibitem[TLI{\etalchar{+}}23]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[TMH{\etalchar{+}}24]{gemmateam2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir~Sanjay Kale, Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology, 2024.

\bibitem[VSP{\etalchar{+}}17]{Vas17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~30, 2017.

\bibitem[XWX{\etalchar{+}}24]{xiao2023florence}
Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce~Liu, and Lu~Yuan.
\newblock Florence-2: Advancing a unified representation for a variety of vision tasks.
\newblock 2024.

\bibitem[YNZ{\etalchar{+}}23]{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2023.

\bibitem[ZBY{\etalchar{+}}24]{zong2024safety}
Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales.
\newblock Safety fine-tuning at (almost) no cost: A baseline for vision large language models.
\newblock {\em arXiv preprint arXiv:2402.02207}, 2024.

\bibitem[ZCG{\etalchar{+}}23]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models, 2023.

\bibitem[ZCS{\etalchar{+}}23]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em arXiv preprint arXiv:2306.05685}, 2023.

\bibitem[ZHB{\etalchar{+}}19]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4791--4800, 2019.

\end{thebibliography}
