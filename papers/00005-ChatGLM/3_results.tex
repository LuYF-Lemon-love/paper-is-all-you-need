\section{GLM-4 Capabilities}



We examine the capabilities of the GLM-4 model from diverse perspectives, including the base capacity on academic benchmarks, code problem-solving, agent abilities in English, and instruction following, long context for both Chinese and English, as well as alignment in Chinese. 
As mentioned, GLM-4 was pre-trained mostly  in Chinese and English and aligned predominantly to Chinese. 
In this section, we report results primarily for the latest GLM-4 version, i.e., GLM-4 (0520) and GLM-4-Air (0605), as GLM-4 (0520) is slightly better than its original 0116 version across the evaluated benchmarks. 
During evaluation, both GLM-4 and GLM-4-Air are deployed with BFloat16 precision.

For baselines, we present results for GPT-4 (0603), GPT-4 Turbo (1106, 2024-04-09), Claude 2, Claude 3 Opus, and Gemini 1.5 Pro, all of which were extracted from the corresponding technical reports or tested through  their public APIs. 

Overall, GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus) in terms of standard benchmarks, as well as instruction following, long context, code problem-solving, and agent abilities in English environment. 
For Chinese alignment, it generates strong performance against SOTA models across various domains, such as fundamental language ability, advanced Chinese understanding, professional knowledge, and open-ended questions.  
In summary, GLM-4 is among the best in terms of Chinese language tasks. 
It also demonstrates comparable performance to GPT-4 and Claude 3 Opus in Chinese math and logic reasoning capabilities though it lags behind GPT-4 Turbo. 








\subsection{Evaluation of Academic Benchmarks}

To evaluate the general performance of the base model, we select six commonly-used benchmarks spanning knowledge, math, reasoning, commonsense, and coding:
\begin{itemize}
    \item MMLU~\cite{hendrycks2021measuring}: Multi-choice questions collected from various examinations including mathematics, history, computer science, and more. We present all answers to the model and ask it to choose the letter of the answer. 
    \item GSM8K~\cite{GSM8k:abs-2110-14168}: 8,500 grade school math word problems (1,000 in the test set) that require the model to solve real-life situational problems using mathematical concepts. We use chain-of-thought prompting~\cite{CoT:Wei0SBIXCLZ22} for this benchmark.
        \item MATH: 12,500 challenging competition-level mathematics problems (5,000 in the test set). We use chain-of-thought prompting~\cite{CoT:Wei0SBIXCLZ22} for this benchmark.
    \item BBH~\cite{BBH:SuzgunSSGTCCLCZ23}: A suite of 23 challenging BIG-Bench~\cite{BigBench:abs-2206-04615} tasks. We use chain-of-thought prompting~\cite{CoT:Wei0SBIXCLZ22} for this benchmark.
    \item GPQA~\cite{GPQA:abs-2311-12022}: A graduate-level multi-choice benchmark in biology, chemistry, and physics. 
    \item HumanEval~\cite{Humaneval:abs-2107-03374}: a coding benchmark that measures correctness of synthetic functions with automatic test-case checking. 
\end{itemize}

We compare the performance of GLM-4 with the original GPT-4~\cite{openai2023gpt}. 
The results are shown in \Cref{tab:academic}. 
We can observe that GLM-4 achieves 96.3\% of GPT-4's accuracy on MMLU, and outperforms GPT-4 on other benchmarks. 
Overall, the base capacity of GLM-4 approaches that of GPT-4-Turbo and Claude 3 Opus.


 
\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \caption{GLM-4 performance on academic benchmarks.}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l|cccccc}
    \toprule
    Model & MMLU & GSM8K & MATH & BBH & GPQA & HumanEval\\
    \midrule
    GPT-4 (0314) & 86.4 & 92.0 & 52.9 & 83.1 & 35.7 & 67.0 \\
    GPT-4 Turbo (1106) & 84.7 & 95.7 & 64.3 & 88.3 & 42.5 & 83.7\\
    GPT-4 Turbo (2024-04-09) & 86.7 & 95.6 & 73.4 & 88.2 & 49.3 & 88.2\\
    % Claude 2\\
    Claude 3 Opus & 86.8 & 95.0 & 60.1 & 86.8 & 50.4 & 84.9 \\
    Gemini 1.5 Pro & 85.9 & 90.8 & 67.7 & 89.2 & 46.2 & 84.1\\
    \midrule
    GLM-4-9B-Chat & 72.4 & 79.6 & 50.6 & 76.3 & 28.8 & 71.8\\
    GLM-4-Air (0605) & 81.9 & 90.9 & 57.9 & 80.4 & 38.4 & 75.7\\
    GLM-4 (0116) & 81.5 & 87.6 & 47.9 & 82.3 & 35.7 & 72.0 \\
    GLM-4 (0520)  & 83.3 & 93.3 & 61.3 & 84.7 & 39.9 & 78.5\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:academic}
\end{table}




\subsection{Evaluation of Instruction Following}

We assess the proficiency of GLM-4 in following instructions with the recently-introduced IFEval dataset~\cite{zhou2023instruction}.
The dataset comprises 541 prompts derived from 25 distinct instructions that are verifiable through explicit criteria (e.g., \emph{``end your email with: P.S. I do like the cake''} can be verified via string matching).
We adhere to the methodologies outlined by \cite{zhou2023instruction} to calculate prompt-level and instruction-level accuracy in both \emph{strict mode} and \emph{loose mode}.
To further evaluate the modelâ€™s performance on following instructions in Chinese, we translate the original prompts into Chinese, omitted instructions that are not applicable in Chinese (such as capitalization), and adjust the scoring scripts to accommodate Chinese data.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \caption{GLM-4 performance on IFEval~\cite{zhou2023instruction}, an LLM instruction following benchmark. `L` stands for `Loose` and `S` stands for `Strict`. `P` stands for `Prompt` and `I` stands for `Instruction`.}
      \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l|cccc|cccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{4}{c}{English} & \multicolumn{4}{c}{Chinese} \\
    & L-P & S-P & L-I & S-I & L-P & S-P & L-I & S-I \\
    \midrule
    GPT-4 (0613) & 79.5 & 77.1 & 85.5 & 83.7 & 72.4 & 68.9 & 80.0 & 75.7 \\
    GPT-4 Turbo (1106) & 79.1 & 75.4 & 85.1 & 82.4 & 74.3 & 69.1 & 80.8 & 76.5\\
    GPT-4 Turbo (2024-04-09) & 84.5 & 81.2 & 88.7 & 85.9 & 79.3 & 72.6 & 84.2 & 79.1\\
    Claude 2 & 75.0 & 58.0 & 81.7 & 67.7 & 57.1 & 46.5 & 64.9 & 55.1\\
    Claude 3 Opus & 90.6 & 85.5 & 93.7 & 90.0 & 78.3 & 73.3 & 84.3 & 80.4\\
    \midrule
    GLM-4-9B-Chat & 73.0 & 69.0 & 80.3 & 77.2 & 73.0 & 69.0 & 80.3 & 77.2 \\
    GLM-4-Air (0605) & 80.4 & 75.2 & 86.1 & 82.3 & 79.3 & 71.2 & 84.0 & 77.3\\
    % GLM-4 (0116) &  \\
    GLM-4 (0520) & 83.7 & 79.1 & 88.7 & 85.0 & 79.7 & 71.9 & 84.2 & 78.0\\
    \bottomrule
    \end{tabular}
    }
    \vspace{1em}
    \label{tab:ifeval}
\end{table}

The English and Chinese sections in \Cref{tab:ifeval} show results in both English and Chinese, respectively.
In \emph{loose mode}, GLM-4 matches instruction-level accuracy achieved by GPT-4 Turbo in both English and Chinese.
In \emph{strict mode}, GLM-4 achieves $99.0\%$ and $98.6\%$ of instruction-level accuracy of GPT-4 Turbo (2024-04-09) in English and Chinese, respectively.


\subsection{Evaluation of Alignment}

AlignBench~\cite{liu2023alignbench} provides an automatic LLMs-as-Judge method to benchmark the alignment of LLMs in Chinese context.
It consists 683 queries spanning 8 different categories, and judges model responses using a GPT-4 based multidimensional rule-calibrated pointwise reference-based scoring method.
We evaluate on AlignBench-v1.1, which more carefully improves the reference generation quality, especially by complementing human-collected evidences from webpages with urls for knowledge-requiring problems that takes up 66.5\% of total queries.
On this version, almost all LLMs achieve lower scores than they do in the previous AlignBench more or less.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \caption{GLM-4 performance on AlignBench~\cite{liu2023alignbench}, an LLM benchmark for alignment in Chinese.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccc|c}
    \toprule
    Model & Math & Logic & Language & Chinese & QA & Writing & Role Play & Professional & Overall \\
    \midrule
    GPT-4 (0613) & 7.54 & 7.17 & 7.82 & 7.02 & 7.39 & 7.67 & 8.20 & 7.29 & 7.46 \\
     GPT-4 Turbo (1106) & 7.85 & 7.66 & 7.90 & 7.22 & 8.24 & 8.53 & 8.46 & 7.95 & 7.90\\
   GPT-4 Turbo (2024-04-09) & 8.32 & 7.67 & 7.60 & 7.57 & 8.37 & 7.75 & 8.18 & 8.59 & 8.00\\
   Claude 2 & 6.39 & 5.85 & 6.75 & 5.72 & 6.68 & 5.87 & 6.86 & 6.56 & 6.26\\
   Claude 3 Opus & 7.27 & 7.11 & 7.94 & 7.71 & 8.21 & 7.61 & 7.73 & 8.02 & 7.53\\
   Gemini 1.5 Pro & 7.07 & 7.77 & 7.31 & 7.22 & 8.55 & 7.83 & 7.79 & 8.52 & 7.47 \\
   \midrule
   GLM-4-9B-Chat & 7.00 & 6.01 & 6.69 & 7.26 & 7.97 & 7.59 & 8.10 & 7.52 & 7.01 \\
   GLM-4-Air (0605) & 7.69 & 6.95 & 7.53 & 8.00 & 7.90 & 8.01 & 8.35 & 8.09 & 7.65\\
   GLM-4 (0116) & 7.20 & 7.20 & 7.60 & 8.19 & 8.45 & 7.88 & 8.05 & 8.56 & 7.66 \\
   GLM-4 (0520) & 7.89 & 7.95 & 8.00 & 7.86 & 8.11 & 8.04 & 8.06 & 8.47 & 8.00\\
   \bottomrule
    \end{tabular}
}
    \label{tab:alignbench}
\end{table}

Results are shown in \Cref{tab:alignbench}.
GLM-4 outperforms GPT-4 Turbo, Claude 3 Opus, and Gemini 1.5 Pro in general, achieves the highest overall score among the baselines.
Especially on Chinese Logic Reasoning and Language Understanding dimensions, GLM-4 significantly outperforms all other powerful models.
These results demonstrate its strong grasping of Chinese language and knowledge.

The current performance gap between GLM-4 and GPT-4 Turbo (2024-04-09) mostly lies in the Mathematics dimension.
We have been employing techniques introduced in ChatGLM-Math~\cite{xu2024chatglmmath} such as self-critique to continuously enhance GLM models' reasoning capabilities.



\subsection{Evaluation of Long Context Handling Abilities}

To obtain the performance of GLM-4 on long text tasks, we carry out evaluations on LongBench-Chat~\cite{bai2024longalign}, a benchmark set with contextual lengths ranging from 10-100k, encompassing a wide range of long text scenarios frequently utilized by users, such as document Q\&A, summarizing, and coding. In our quest to provide a more detailed comparison of the performance of GLM-4 in different languages, we also segregate LongBench-Chat according to language. This yields two distinct portions: Chinese and English. We have accordingly supplied the results for both segments separately, offering a fine-grained overview of GLM-4's cross-linguistic capabilities.

Regarding the specific evaluation settings, we score the outputs of each model based on GPT-4, adopting a few-shot strategy within LongBench-Chat. Moreover, given our objective to minimize score variations and to reach a more reliable statistical conclusion, we conduct repeated evaluations. Subsequently, we compute  the average  from these multiple evaluations to ensure that the final performance metric reflects a thorough understanding of how GLM-4 behaves under diverse conditions.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \caption{GLM-4 performance on LongBench-Chat~\cite{bai2023longbench}.}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l|cc}
    \toprule
       Model  & English & Chinese \\
    \midrule
       GPT-4 Turbo (1106) & 87.2 & 71.4\\
       GPT-4 Turbo (2024-04-09) & 85.0 & 82.1\\
       Claude 2 & 81.3 & 76.2 \\
       Claude 3 Opus & 87.7 & 82.7 \\
       \midrule
       GLM-4-9B-Chat & 76.8 & 79.0\\
       GLM-4-Air (0605) & 82.4 & 81.0\\
       GLM-4 (0520)  & 87.3 & 84.0 \\
       \bottomrule
    \end{tabular}
    }
    \label{tab:longbench}
\end{table}

\Cref{tab:longbench} features the results obtained from our experiments. 
It can be clearly observed that the performance of GLM-4 aligns with that of GPT-4 Turbo and Claude 3 Opus on English prompts, and it can  outperform the best of them on Chinese prompts. 




\subsection{Evaluation of Coding on Real-world User Prompts}

While HumanEval~\cite{Humaneval:abs-2107-03374} has been widely adopted for evaluating code generation, most of its problems can be categorized to introductory algorithms.
However, in practice, real users ask complicated questions for production purposes, which are usually far beyond the scope of HumanEval.
Additionally, previous works have reported HumanEval-contaminated training data~\cite{openai2023gpt,li2023textbooks,yang2023rethinking} in their own or other LLMs, making the results on HumanEval relatively less trustful than before.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \caption{GLM-4 performance on NaturalCodeBench (NCB)~\cite{zhang2024naturalcodebench}, a benchmark with real coding prompts in two programming languages (Python and Java) for English and Chinese.}
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l|cccc|c}
    \toprule
    Model  &  Python (en) & Java (en) & Python (zh) & Java (zh) & Overall\\
    \midrule
    GPT-4 (0613) & 55.7 & 51.1 & 53.4 & 51.1 & 52.8 \\
    GPT-4 Turbo (1106) & 51.9 & 55.0 & 47.3 & 51.9 & 51.5\\
    GPT-4 Turbo (2024-04-09) & 57.5 & 52.3 & 53.1 & 52.3 & 53.8\\
    Claude 2 & 34.4 & 36.6 & 33.6 & 32.8 & 34.4\\
    Claude 3 Opus & 48.9 & 48.9 & 45.0 & 50.4 & 48.3\\
    Gemini 1.5 Pro & 45.0 & 39.7 & 41.5 & 43.1 & 42.3 \\
    \midrule
    GLM-4-9B-Chat & 33.9 & 29.8 & 30.8 & 34.4 & 32.2 \\
    GLM-4-Air (0605) & 40.8 & 39.7 & 43.1 & 39.7 & 40.8\\
    GLM-4 (0520) & 51.6 & 42.8 & 45.4 & 48.9 & 47.1\\
       \bottomrule
    \end{tabular}
    }
    \label{tab:ncb}
\end{table}



As a result, beside HumanEval we evaluate GLM-4 on NaturalCodeBench (NCB)~\cite{zhang2024naturalcodebench}, a challenging bilingual coding benchmark derived from natural user prompts to mirror the complexity of real-world coding missions.
Results are shown in \Cref{tab:ncb}. 
It shows that GLM-4 has a close coding performance to Claude 3 Opus in practical scenarios.
While there is still some gaps to GPT-4 models, considering GLM-4 bilingually balanced nature, there is  quite much potential to improve its performance on NCB via better training strategies and data curation in our following iterations.






\subsection{Evaluation of Function Call}

To evaluate the performance of GLM models on function call, we carry out evaluations on Berkeley Function Call Leaderboard~\cite{berkeley-function-calling-leaderboard}, a benchmark with 2k question-function-answer pairs. The benchmark evaluates model capacity on calling functions in three categories: evaluation by Abstract Syntax Tree (AST), evaluation by executing APIs, and relevance detection. The first category compares the model output functions against function documents and possible answers with AST analysis. The second category checks for response correctness by executing the generated function calls. Relevance detection evaluates the model's capacity on recognizing functions that are not suitable to answer the user's question. 
The results are shown in \Cref{tab:fc_result}. We can observe that the function-call capacity of  GLM-4 (0520) aligns with that of GPT-4 Turbo (2024-04-09), while GLM-4-9B-Chat significantly outperforms Llama-3-8B-Instruct. Another observation is that the overall accuracy does not improve with model sizes, while GLM-4-9B-Chat can even outperform GLM-4-Air. 
On the other hand, we observe that the performance on execution summary, which evaluates the execution results of real-world APIs, improves smoothly with model sizes.

\input{tables/fc_table}

\subsection{Evaluation of Agent Abilities}


It is widely observed that LLMs are capable to serve as intelligent agents in versatile environments and contexts~\cite{park2023generative,yao2022react}, known as LLMs-as-Agents~\cite{liu2023agentbench}.
As a result, we evaluate GLM-4 together with other comparison LLMs on AgentBench~\cite{liu2023agentbench}, a comprehensive agentic benchmark for text-based LLMs across an array of practical environments, including code-based, game-based, and web-based contexts.
Specifically, we evaluate on 7 out of 8 AgentBench environments except for Digital Card Game, which takes much longer time to interact with.
Overall scores are calculated using the original per-dataset weights provided in AgentBench~\cite{liu2023agentbench}.

\input{tables/agent_table}

The results are presented in Table~\ref{tab:agent}.
As it shows, GLM-4 models present quite impressive performance on agent tasks, with the GLM-4-Air's comparable and GLM-4's outperforming scores to GPT-4 Turbo and Claude 3 Opus.
In terms of specific environments, we find GLM-4 series performing especially well on Database, House-Holding, and Web Shopping tasks, while still demonstrating a gap to GPT-4 series on Operating System, Knowledge Graph, and Lateral Thinking Puzzles.
The gap indicates that there is still some room for GLM-4 to improve its performance on code-related agentic tasks and highly interactive language tasks.

\subsection{Evaluation of All Tools}

GLM-4 is further aligned to support intelligent agents and user self-configured GLMs functionalities on \url{https://chatglm.cn}, and the resultant model is GLM-4 All Tools. 
As mentioned, GLM-4 All Tools can  complete complex tasks by autonomously understanding user intent, planing step-by-step instructions, and calling multiple tools, including web browser, Python interpreter, and the text-to-image model (e.g., CogView3~\cite{zheng2024cogview3}. 
\Cref{tb:alltools} shows that GLM-4 All Tools (Web) can generate similar performance on Python Interpreter for solving math problems, browser for information seeking, compared to ChatGPT-4 (Web), respectively. 

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.5}
\caption{Performance of GLM-4 All Tools.}
\resizebox{0.65\columnwidth}{!}{
\begin{tabular}{c|c|cc}
\toprule
 & & {GLM-4 All Tools}  & {GPT-4}\\
 & & (Web, 0116) & (Web, 0110)\\
\midrule
\multirow{3}{*}{\shortstack[l]{{Python }\\{Interpreter}   }}  
 & GSM8K    & 91.59  & 92.72   \\
 & MATH     & 63.60  & 65.00   \\
 & Math23K  & 88.50  & 88.40   \\
 \midrule
\multirow{1}{*}{\shortstack[l]{{Browser } }}  
& Information Seeking & 78.08  & 67.12 \\

\bottomrule
\end{tabular}
}
\label{tb:alltools}
\end{table}
