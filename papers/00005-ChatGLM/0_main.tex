\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage[nonatbib]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table,xcdraw]{xcolor}         % colors

\usepackage{tabularx,booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% my packages
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xspace}
\usepackage{enumitem}
% \usepackage{float}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts,amssymb}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{natbib}
\setcitestyle{numbers,square}
\usepackage{graphicx}       % graphs
\usepackage{cleveref}
\usepackage{hyperref}
\usepackage{ulem}

% my commands
\newcommand{\hhide}[1]{}
\newcommand{\hide}[1]{}
\newcommand{\model}[0]{ChatGLM\xspace}
\newcommand{\vpara}[1]{\vspace{0.07in}\noindent\textbf{#1}\xspace} % 
\newcommand{\todo}[1]{{\color{red}[\textbf{(2do: #1 )]}}}
\newcommand{\dong}[1]{{\color{blue}[(Dong: #1 )]}}


\title{ChatGLM: A Family of Large Language Models \\from GLM-130B to GLM-4 All Tools}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
    XXX \thanks{The original paper is located at \url{https://arxiv.org/abs/2406.12793}. This version has been modified by LuYF-Lemon-love $\langle$ \url{luyanfeng_nlp@qq.com} $\rangle$ for personal study.}\\
    Tsinghua University
}

\begin{document}


\maketitle

\begin{abstract}

We introduce ChatGLM, an evolving family of large language models that we have been developing over time. 
This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. 
To date, the GLM-4 models are pre-trained on \uline{ten trillions of tokens} mostly in \uline{Chinese} and \uline{English}, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. 
The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. 
The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) to use---including \uline{web browser}, \uline{Python interpreter}, \uline{text-to-image model}, and \uline{user-defined functions}---to effectively complete complex tasks.
Over the course, we have open-sourced a series of models, including \uline{ChatGLM-6B} (three generations), \uline{GLM-4-9B} (128K, 1M), \uline{GLM-4V-9B}, \uline{WebGLM}, and \uline{CodeGeeX}, attracting \uline{over 10 million downloads} on Hugging face in the year 2023 alone. 
The open models can be accessed through \url{https://github.com/THUDM} and \url{https://huggingface.co/THUDM}. 

\end{abstract}
\clearpage

\input{1_intro}
\input{2_method}
\input{3_results}
\input{4_safety}
\input{5_conclusion}

\vpara{Acknowledgement.} 
We would like to thank all the data annotators, infra operating staffs, collaborators, and partners as well as everyone at Zhipu AI and Tsinghua University not explicitly mentioned in the report who have provided support, feedback, and contributed to ChatGLM. 
We would also like to thank Yuxuan Zhang and Wei Jia from Zhipu AI as well as the teams at Hugging Face, ModelScope, WiseModel, and others for their help on the open-sourcing efforts of the GLM family of models.

\bibliographystyle{abbrv}
\bibliography{ref}

\end{document}