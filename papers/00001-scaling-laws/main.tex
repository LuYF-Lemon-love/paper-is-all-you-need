%%%%%%%%%%%%%%%%%%%%%%%%
%%% LaTeX Stuff Here %%%
%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[english]{article}
\pdfsuppresswarningpagegroup=1

\usepackage[preprint,nonatbib]{nips_2018_wider_nonotice}

\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{color,colortbl}
\usepackage{babel}
\usepackage{verbatim}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{cancel}
\hypersetup{unicode=true,pdfusetitle,breaklinks=false}
\usepackage[hyperpageref]{backref}
\usepackage{appendix}

\graphicspath{{figures/}}

\usepackage[labelfont=bf]{caption}
\captionsetup[figure]{labelsep=quad}
\captionsetup[table]{labelsep=quad}

\renewcommand{\arraystretch}{1.5}

\makeatletter
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\CB}{{\cal B}}
\newcommand{\blue}{\color{blue}}
\allowdisplaybreaks \numberwithin{equation}{section}
% \usepackage{refcheck}
\setcounter{tocdepth}{1}
\makeatother


%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Paper starts here %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Scaling Laws for Neural Language Models}

\author{
    XXX \thanks{The original paper is located at \url{https://arxiv.org/abs/2001.08361}. This version has been modified by LuYF-Lemon-love <\url{luyanfeng_nlp@qq.com}> for personal study.}\\
    OpenAI
}

\begin{document}
\maketitle

\begin{abstract}
\textbf{We study empirical scaling laws for language model performance on the cross-entropy loss.}
The loss scales as a \textbf{power-law} with \textbf{model size}, \textbf{dataset size}, and \textbf{the amount of compute used for training}, with some trends spanning more than seven orders of magnitude.
Other architectural details such as network width or depth have minimal effects within a wide range.
\textbf{Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.}

\end{abstract}

% \newpage\tableofcontents{}
\tableofcontents{}

\section{Introduction}

\subsection{Summary}

Our key findings for Transformer language models are are as follows:

\paragraph{Performance depends strongly on scale, weakly on model shape:} Model performance depends most strongly on scale, which consists of three factors: the number of model parameters $N$ (excluding embeddings), the size of the dataset $D$, and the amount of compute $C$ used for training.  Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs.~width. (Section \ref{sec:Empirical})

\paragraph{Smooth power laws:} Performance has a power-law relationship with each of the three scale factors $N, D, C$ when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure \ref{fig:BasicPowerLaws}). (Section \ref{sec:Empirical})

\begin{figure}
\includegraphics[width=\textwidth]{SimplePowerLaws}

\caption[Summary of simple power laws.]{Language modeling performance improves smoothly as we increase the model size, datasetset size, and amount of compute used for training.  For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two.  \label{fig:BasicPowerLaws}}
\end{figure}

\paragraph{Universality of overfitting:} Performance improves predictably as long as we scale up $N$ and $D$ in tandem, but enters a regime of diminishing returns if either $N$ or $D$ is held fixed while the other increases.  \textbf{The performance penalty depends predictably on the ratio $N^{0.74}/D$, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty.} (Section \ref{sec:ChartingOverfitting})

\paragraph{Universality of training:} Training curves follow predictable power-laws whose parameters are roughly independent of the model size.  By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. (Section \ref{sec:ScalingSizeandSteps})

\paragraph{Transfer improves with test performance:} When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss -- in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set. (Section \ref{sec:GeneralizationtoOtherDistributions})

\paragraph{Sample efficiency:} \textbf{Large models are  more sample-efficient than small models, reaching the same level of performance with fewer optimization steps (Figure \ref{fig:EfficiencyIllustration}) and using  fewer data points (Figure \ref{fig:LossvsModelDatasetSize}).}


\begin{figure}
\noindent \centering{} \includegraphics[width=0.99\textwidth]{EfficiencyIllustration}
\caption[Illustration of sample efficiency and compute efficiency.]{We show a series of language model training runs, with models ranging in size from $10^3$ to $10^9$ parameters (excluding embeddings). \label{fig:EfficiencyIllustration}}
\end{figure}

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.48\textwidth]{LossvsModelDatasetSize}
\includegraphics[width=0.48\textwidth]{LearningCurveFitComparisonIntro}
\caption[Performance when varying model and data size, or model and training steps, simultaneously]{
{\bf Left}: The early-stopped test loss $L(N, D)$ varies predictably with the dataset size $D$ and model size $N$ according to Equation \eqref{eq:FundamentalLikelihioodvsModelandDataSize}.  
{\bf Right}:  After an initial transient period, learning curves for all model sizes $N$ can be fit with Equation \eqref{eq:FundamentalLikelihioodvsModelandSteps}, which is parameterized in terms of $S_{\rm min}$, the number of steps when training at large batch size (details in Section \ref{sec:OptimalBatchSize}).  \label{fig:LearningCurveFitsandResiduals}
\label{fig:LossvsModelDatasetSize}}
\end{figure}

\paragraph{Convergence is inefficient:} When working within a fixed compute budget $C$ but without any other restrictions on the model size $N$ or available data $D$, we attain optimal performance by training \emph{very large models} and stopping \emph{significantly short of convergence} (see Figure \ref{fig:ContributionIllustration}).  Maximally compute-efficient training would therefore be far more sample efficient than one might expect based on training small models to convergence, with data requirements growing very slowly as $D \sim C^{0.27}$ with training compute. (Section \ref{sec:OptimalCompute})

\begin{figure}
\noindent \centering{} \includegraphics[height=0.32\textwidth]{ContributionIllustration}
\caption[How to scale up model size, batch size, and serial steps]{As more compute becomes available, we can choose how much to allocate towards training larger models, using larger batches, and training for more steps.  We illustrate this for a billion-fold increase in compute.  For optimally compute-efficient training, most of the increase should go towards increased model size.  A relatively small increase in data is needed to avoid reuse.  Of the increase in data, most can be used to increase parallelism through larger batch sizes, with only a very small increase in serial training time required.  \label{fig:ContributionIllustration} }
\end{figure}

\paragraph{Optimal batch size:} The ideal batch size for training these models is roughly a power of the loss only. (Section \ref{sec:OptimalBatchSize})

\subsection{Summary of Scaling Laws}

The test loss of a Transformer trained to autoregressively model language can be predicted using a power-law  when performance is limited by only either the number of non-embedding parameters $N$, the dataset size $D$, or the optimally allocated compute budget $C_{\rm min}$ (see Figure \ref{fig:BasicPowerLaws}):
\begin{enumerate}
\setlength\itemsep{0.5em}
\item For models with a limited number of parameters, trained to convergence on sufficiently large datasets:
\be L(N) = \left(N_{\mathrm{c}}/N\right)^{\alpha_N};~~ \alpha_N \sim 0.076, \quad N_{\mathrm{c}} \sim 8.8 \times 10^{13}~\text{(non-embedding parameters)} \label{eq:crit_n} \ee
\item For large models trained with a limited dataset with early stopping:
\be L(D) = \left(D_{\mathrm{c}}/D\right)^{\alpha_D};~~ \alpha_D \sim 0.095, \quad D_{\mathrm{c}} \sim 5.4 \times 10^{13}~\text{(tokens)} \label{eq:crit_d} \ee
\item When training with a limited amount of compute, a sufficiently large dataset, an optimally-sized model, and a sufficiently small batch size:
\be L(C_{\rm min}) = \left(C_{\mathrm{c}}^{\rm min} / C_{\rm min}\right)^{\alpha_C^{\rm min}};~~ \alpha_C^{\rm min} \sim 0.050, \quad C_{\mathrm{c}}^{\rm min} \sim 3.1 \times 10^{8}~\text{(PF-days)} \label{eq:crit_c} \ee
\end{enumerate}

They depend very weakly on model shape and other Transformer hyperparameters (depth, width, number of self-attention heads), with specific numerical values associated with the Webtext2 training set \cite{radford2019language}.
\textbf{The power laws $\alpha_{\rm N}, \alpha_{\rm D}, \alpha_{C}^{\rm min}$ specify the degree of performance improvement expected as we scale up $N$, $D$, or $C_{\rm min}$; for example, doubling the number of parameters yields a loss that is smaller by a factor $2^{-\alpha_N}=0.95$.}
The precise numerical values of $N_{\mathrm{c}}, C_{\rm c}^{\rm min},$ and $D_{\mathrm{c}}$ depend on the vocabulary size and tokenization and hence do not have a fundamental meaning.  


The critical batch size, which determines the speed/efficiency tradeoff for data parallelism (\cite{1812.06162}), also roughly obeys a power law in $L$:
\begin{equation}
B_{\rm crit}\left(L\right)=\frac{B_{\ast}}{L^{1/\alpha_{B}}},\qquad B_{\ast} \sim 2\cdot 10^8 \text{ tokens},\ \ \alpha_{B} \sim 0.21
\label{eq:critical-batch-size}
\end{equation}

Equation \eqref{eq:crit_n} and \eqref{eq:crit_d} together suggest that as we increase the model size, we should increase the dataset size sublinearly according to $D \propto N^{\frac{\alpha_N}{\alpha_D}} \sim N^{0.74}$. In fact, we find that there is a single equation combining \eqref{eq:crit_n} and \eqref{eq:crit_d} that governs the simultaneous dependence on $N$ and $D$ and governs the degree of overfitting:
\be
\label{eq:FundamentalLikelihioodvsModelandDataSize}
L(N, D) 
= \left[ \left( \frac{N_c}{N} \right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D_c}{D}  \right]^{\alpha_D}
\ee
with fits pictured on the left in figure \ref{fig:LossvsModelDatasetSize}.

When training a given model for a finite number of parameter update steps $S$ in the infinite data limit, after an initial transient period, the learning curves can be accurately fit by  (see the right of figure \ref{fig:LearningCurveFitsandResiduals})
\be
\label{eq:FundamentalLikelihioodvsModelandSteps}
L(N, S) = \left( \frac{N_c}{N} \right)^{\alpha_N}  + \left( \frac{S_c }{S_{\rm min}(S)} \right)^{\alpha_S}
\ee
where $S_c \approx 2.1 \times 10^3$ and $\alpha_S \approx 0.76$, and $S_{\rm min}(S)$ is the minimum possible number of optimization steps (parameter updates)
estimated  using Equation \eqref{eq:AdjustedSteps}.

When training within a fixed compute budget $C$, but with no other constraints, Equation \eqref{eq:FundamentalLikelihioodvsModelandSteps} leads to the prediction that the optimal model size $N$, optimal batch size $B$, optimal number of steps $S$, and dataset size $D$ should grow as 
\be
\label{eq:OptimalModelSizeTrainingTimevsCompute}
N \propto C^{\alpha_{C}^{\rm min} /\alpha_{N}}, \quad
B \propto C^{\alpha_C^{\rm min} / \alpha_B}, \quad
S \propto C^{\alpha_C^{\rm min} / \alpha_S}, \quad
D = B\cdot S \quad
\ee
with
\be
\alpha_{C}^{\rm min} = 1/\left(1/\alpha_{S}+1/\alpha_{B}+1/\alpha_{N}\right) 
\ee
which closely matches the empirically optimal results $N \propto C_{\rm min}^{0.73}$, $B \propto C_{\rm min}^{0.24}$, and $S \propto C_{\rm min}^{0.03}$.
\textbf{As the computational budget $C$ increases, it should be spent primarily on larger models, without dramatic increases in training time or dataset size (see Figure \ref{fig:ContributionIllustration}).}

\subsection{Notation}

We use the following notation:
\begin{itemize}
\item $L$ -- the cross entropy loss in nats.
\item $N$ -- the number of model parameters, \emph{excluding all vocabulary and positional embeddings}  
\item $C \approx 6 N B S$ -- an estimate of the total non-embedding training compute, where $B$ is the batch size, and $S$ is the number of training steps (ie parameter updates). We quote numerical values in PF-days, where one PF-day $ = 10^{15} \times 24 \times 3600 = 8.64 \times 10^{19}$ floating point operations.  
\item $D$ -- the dataset size in tokens
\item $B_{\rm crit}$ -- the critical batch size \cite{1812.06162}, defined and discussed in Section \ref{sec:OptimalBatchSize}.  Training at the critical batch size provides a roughly optimal compromise between time and compute efficiency.
\item $C_{\rm min}$ -- an estimate of the minimum amount of non-embedding compute to reach a given value of the loss.  \textbf{This is the training compute that would be used if the model were trained at a batch size much less than the critical batch size.}  
\item $S_{\rm min}$ -- an estimate of the minimal number of training steps needed to reach a given value of the loss.  \textbf{This is also the number of training steps that would be used if the model were trained at a batch size much greater than the critical batch size.}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We train language models on WebText2, an extended version of the WebText \cite{radford2019language} dataset, tokenized using byte-pair encoding \cite{BPE} with a vocabulary size $n_{\rm vocab} = 50257$.  We optimize the autoregressive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal performance metric.  We primarily train decoder-only \cite{liu2018generating, radford2018improving} Transformer \cite{OriginalTransformer} models.

\subsection{Parameter and Compute Scaling of Transformers}
\label{sec:ParameterComputeCounts}

We parameterize the Transformer architecture using hyperparameters $n_{\rm layer}$ (number of layers), $d_{{\rm model}}$ (dimension of the residual stream), $d_{\rm ff}$ (dimension of the intermediate feed-forward layer), $d_{\rm attn}$ (dimension of the attention output), and $n_{\rm heads}$ (number of attention heads per layer).  We include $n_{\rm ctx}$ tokens in the input context, with $n_{\rm ctx} = 1024$ except where otherwise noted.

We use $N$ to denote the model size, which we define as the number of \emph{non-embedding} parameters
\begin{align}
\label{eq:ModelSizeDefinition}
N & \approx  2d_{{\rm model}}n_{{\rm layer}}\left(2d_{{\rm attn}}+d_{{\rm ff}}\right) & \nonumber \\
& = 12 n_{\rm layer} d_{{\rm model}}^2 \quad \text{ with the standard } \quad d_{\rm attn} = d_{\rm ff}/4 = d_{{\rm model}} &
\end{align}
where we have excluded biases and other sub-leading terms.
Our models also have $n_{\rm vocab} d_{{\rm model}}$ parameters in an embedding matrix, and use $n_{\rm ctx} d_{{\rm model}}$ parameters for positional embeddings, but we do not include these when discussing the `model size' $N$; we will see that this produces significantly cleaner scaling laws.

Evaluating a forward pass of the Transformer involves roughly
\be
\label{eq:ApproximateTotalCompute}
C_{\rm forward} \approx 2N + 2n_{{\rm layer}}n_{{\rm ctx}}d_{{\rm model}}
\ee
add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in matrix multiplication.  A more detailed per-operation parameter and compute count is included in Table \ref{tab:TableTransformerParamsFLOPs}.

\begin{table}[t!]
\centering
\begin{tabular}{|l|l|l|}
\hline 
\textbf{Operation}  & \textbf{Parameters}  & \textbf{FLOPs per Token}\tabularnewline
\hline 
\hline 
Embed  & $\left(n_{{\rm vocab}} + n_{{\rm ctx}}\right)d_{{\rm model}}$  & $4d_{{\rm model}}$\tabularnewline
\hline 
Attention: QKV  & $n_{{\rm layer}}d_{{\rm model}}3d_{{\rm attn}}$  & $2n_{{\rm layer}}d_{{\rm model}}3d_{{\rm attn}}$\tabularnewline
\hline 
Attention: Mask & ---  & $2n_{{\rm layer}}n_{{\rm ctx}}d_{{\rm attn}}$\tabularnewline
\hline 
Attention: Project & $n_{{\rm layer}}d_{{\rm attn}}d_{{\rm model}}$  & $2n_{{\rm layer}}d_{{\rm attn}}d_{{\rm embd}}$\tabularnewline
\hline 
Feedforward  & $n_{{\rm layer}}2d_{{\rm model}}d_{{\rm ff}}$ & $2n_{{\rm layer}}2d_{{\rm model}}d_{{\rm ff}}$\tabularnewline
\hline 
De-embed  & ---  & $2d_{{\rm model}}n_{{\rm vocab}}$\tabularnewline
\hline 
\hline 
\textbf{Total (Non-Embedding)} & $N=2d_{{\rm model}}n_{{\rm layer}}\left(2d_{{\rm attn}}+d_{{\rm ff}}\right)$  & $C_{\mathrm{forward}}=2N+2n_{{\rm layer}}n_{{\rm ctx}}d_{{\rm attn}}$\tabularnewline
\hline 
\end{tabular}
\vspace{1em}
\caption[Parameter and compute counts for Transformer]{Parameter counts and compute (forward pass) estimates for a Transformer model.  Sub-leading terms such as nonlinearities, biases, and layer normalization are omitted. \label{tab:TableTransformerParamsFLOPs}}
\end{table}

For contexts and models with $d_{{\rm model}} > n_{\rm ctx} / 12$, the context-dependent computational cost per token is a relatively small fraction of the total compute.
Since we primarily study models where $d_{\rm model} \gg n_{\rm ctx}/12$, we do not include context-dependent terms in our training compute estimate.  Accounting for the backwards pass (approximately twice the compute as the forwards pass), we then define the estimated non-embedding compute as $C \approx 6 N$ floating point operators per training token.


\subsection{Training Procedures}

Unless otherwise noted, we train  models with the Adam optimizer \cite{kingma2014adam} for a fixed $2.5 \times 10^5$  steps with a batch size of $512$ sequences of $1024$ tokens.
Due to memory constraints, our largest models (more than 1B parameters) were trained with Adafactor \cite{DBLP:journals/corr/abs-1804-04235}.
We found that results at convergence were largely independent of learning rate schedule.  Unless otherwise noted, all training runs included in our data used a learning rate schedule with a 3000 step linear warmup followed by a cosine decay to zero.

\subsection{Datasets}
We train our models on an extended version of the WebText dataset described in \cite{radford2019language}.  The original WebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at least 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January to October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether people found the link interesting or useful. The text of the new links was extracted with the Newspaper3k python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and $1.62 \times 10^{10}$ words (as defined by \texttt{wc}). We then apply the reversible tokenizer described in \cite{radford2019language}, which yields $2.29 \times 10^{10}$ tokens.  We reserve $6.6 \times 10^{8}$ of these tokens for use as a test set, and we also test on similarly-prepared samples of Books Corpus \cite{Zhu_2015}, Common Crawl \cite{commoncrawl}, English Wikipedia, and a collection of publicly-available Internet Books.



\section{Empirical Results and Basic Power Laws}
\label{sec:Empirical}

To characterize language model scaling we train a wide variety of models, varying a number of factors including:
\begingroup
\renewcommand{\arraystretch}{1.1}
\begin{itemize}
\item Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)
\item Dataset size (ranging from 22 million to 23 billion tokens)
\item Shape (including depth, width, attention heads, and feed-forward dimension)
\item Context length (1024 for most runs, though we also experiment with shorter contexts)
\item Batch size ($2^{19}$ for most runs, but we also vary it to measure the critical batch size)
\end{itemize}
\endgroup 

\subsection{Approximate Transformer Shape and Hyperparameter Independence}
\label{sec:ShapeIndependence}

Transformer performance depends very weakly on the shape parameters $n_{\rm layer}, n_{\rm heads}$, and $d_{\rm ff}$ when we hold the total non-embedding parameter count $N$ fixed.  
The results are shown in Figure \ref{fig:HeadsLayersIndependence}.

\begin{figure}
\includegraphics[width=\textwidth]{HyperparameterTuning}
 \caption[Weak dependence of performance on hyperparameter tuning]{Performance depends very mildly on model shape when the total number of non-embedding parameters $N$ is held fixed.  The loss varies only a few percent over a wide range of shapes.  Small differences in parameter counts are compensated for by using the fit to $L(N)$ as a baseline.  Aspect ratio in particular can vary by a factor of 40 while only slightly impacting performance; an $(n_{\mathrm{layer}}, d_{\mathrm{model}}) = (6, 4288)$ reaches a loss within 3\% of the $(48, 1600)$ model used in \cite{radford2019language}.  \label{fig:HeadsLayersIndependence}}
\end{figure}

\subsection{Performance with Non-Embedding Parameter Count $N$} 
\label{sec:PerformancevsModelSize}

As shown in  Figure \ref{fig:BasicPowerLaws}, we find a steady trend with non-embedding parameter count $N$, which can be fit to the first term of Equation \eqref{eq:FundamentalLikelihioodvsModelandDataSize}, so that
\be
L(N) \approx \left( \frac{N_c}{N} \right)^{ \alpha_N }
\ee  
To observe these trends it is crucial to study performance as a function of $N$; if we instead use the total parameter count (including the embedding parameters) the trend is somewhat obscured (see Figure \ref{fig:PerformancevsModelSizeBody}).  This suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in recent work \cite{lan2019albert}.

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.45\textwidth]{PerfVsModelSizeAllParams}
\includegraphics[width=0.45\textwidth]{PerfVsModelSizeNonEmbed}
\caption[Comparison of performance trend when including or excluding embeddings]{
{\bf Left:} When we include embedding parameters, performance appears to depend strongly on the number of layers in addition to the number of parameters.
{\bf Right:} When we exclude embedding parameters, the performance of models with different depths converge to a single trend. Only models with fewer than 2 layers or with extreme depth-to-width ratios deviate significantly from the trend.  \label{fig:PerformancevsModelSizeBody}}
\end{figure}

Although these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets is also a power-law in $N$ with nearly identical power, as shown in Figure \ref{fig:GeneralizationVsModelSize}.

\begin{figure}
\noindent \centering{} \includegraphics[width=0.48\textwidth]{GeneralizationVsModelSize}\hfill
\noindent \centering{} \includegraphics[width=0.48\textwidth]{TrainingVsConvergence}
\caption[Generalization to other test datasets]{
\textbf{Left:} Generalization performance to other data distributions improves smoothly with model size, with only a small and very slowly growing offset from the WebText2 training distribution.
\textbf{Right:} Generalization performance depends only on training distribution performance, and not on the phase of training.  We compare generalization of converged models (points) to that of a single large model (dashed curves) as it trains.
\label{fig:GeneralizationVsModelSize}}
\end{figure}

\subsubsection{Generalization Among Data Distributions}
\label{sec:GeneralizationtoOtherDistributions}


We have also tested our models on a set of additional text data distributions.  The test loss on these datasets as a function of model size is shown in Figure \ref{fig:GeneralizationVsModelSize}; in all cases the models were trained only on the WebText2 dataset.  \textbf{We see that the loss on these other data distributions improves smoothly with model size, in direct parallel with the improvement on WebText2.}  We find that generalization depends almost exclusively on the in-distribution validation loss, and does not depend on the duration of training or proximity to convergence. We also observe no dependence on model depth (see Figure \ref{fig:DepthVsGeneralization}).

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.5\textwidth]{DepthVsGeneralization}
\caption[Generalization versus depth]{We show evaluations on a series of datasets for models with approximately 1.5 Billion parameters.  We observe no effect of depth on generalization; generalization performance depends primarily on training distribution performance. The 12-layer model overfit the Internet Books dataset and we show the early-stopped performance; we have not seen this surprising result in other experiments. \label{fig:DepthVsGeneralization}}
\end{figure}

\subsection{Performance with Dataset Size and Compute}

We display empirical trends for the test loss as a function of dataset size $D$ (in tokens) and training compute $C$ in Figure \ref{fig:BasicPowerLaws}.    

We stopped training once the test loss ceased to decrease.  We see that the resulting test losses can be fit with simple power-law
\be
L(D) \approx \left( \frac{D_c}{D} \right)^{\alpha_D}
\ee
in the dataset size.  The data and fit appear in Figure \ref{fig:BasicPowerLaws}.

The total amount of non-embedding compute used during training can be estimated as $C = 6 N B S$, where $B$ is the batch size, $S$ is the number of parameter updates, and the factor of $6$ accounts for the forward and backward passes.  Thus for a given value of $C$ we can scan over all models with various $N$ to find the model with the best performance on step $S = \frac{C}{6 B S}$.  Note that in these results \emph{the batch size $B$ remains fixed for all models}, which means that these empirical results are not truly optimal.

The result appears as the heavy black line on the left-hand plot in Figure \ref{fig:BasicPowerLaws}.  It can  be fit with 
\be
L(C) \approx \left( \frac{C_c}{C} \right)^{\alpha_C}
\ee

\section{Charting the Infinite Data Limit and Overfitting}
\label{sec:ChartingOverfitting}

\subsection{Results}

We regularize all our models with 10\% dropout, and by tracking test loss and stopping once it is no longer decreasing.   The results are displayed in Figure \ref{fig:DatasetModelSizevsPerformance}, including a fit to the four parameters $\alpha_N, \alpha_D, N_c, D_c$ in Equation \eqref{eq:FundamentalLikelihioodvsModelandDataSize}:

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.48\textwidth]{DatasetModelSizevsPerformance}\hfill
\includegraphics[width=0.48\textwidth]{DatasetModelSizevsChangePerformance}
\caption[Universality of overfitting]{
The early-stopped test loss $L(N, D)$ depends predictably on the dataset size $D$ and model size $N$ according to Equation \eqref{eq:FundamentalLikelihioodvsModelandDataSize}.
{\bf Left}:
For large $D$, performance is a straight power law in $N$. For a smaller fixed $D$, performance stops improving as $N$ increases and the model begins to overfit. (The reverse is also true, see Figure \ref{fig:LossvsModelDatasetSize}.)
{\bf Right}:  The extent of overfitting depends predominantly on the ratio $N^{\frac{\alpha_N}{\alpha_D}}/D$, as predicted in equation (\ref{eq:OverfittingPrediction}).  The line is our fit to that equation.
\label{fig:DatasetModelSizevsPerformance}}
\end{figure}

\begin{table}[h!]
\centering
\vspace{-0.5em}
\begin{tabular}{|c| c | c | c | c|} 
 \hline
Parameter & $\alpha_N$ & $\alpha_D$ & $N_c$ & $D_c$ \\ [0.5ex] 
 \hline\hline
Value  & $0.076$ & $0.103$ & $6.4 \times 10^{13}$ & $1.8 \times 10^{13}$ \\ 
 \hline
\end{tabular}
\vspace{0.5em}
\caption{Fits to $L(N, D)$}
\vspace{-1em}
\end{table}

To chart the borderlands of the infinite data limit, we can directly study the extent of overfitting.  For all but the largest models, we see no sign of overfitting when training with the full 22B token WebText2 dataset, so we can take it as representative of $D=\infty$.  Thus we can compare finite $D$ to the infinite data limit by defining
\be
\delta L(N, D) \equiv \frac{L(N, D)}{L(N, \infty)} - 1
\ee
and studying it as a function of $N, D$.   In fact, we see empirically that $\delta L$ depends only a specific combination of $N$ and $D$, as shown in Figure \ref{fig:OverfittingandEarlyStopping}.  This follows from the scaling law of Equation \eqref{eq:FundamentalLikelihioodvsModelandDataSize}, which implies

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.48\textwidth]{EarlyStoppingvsNandD}\hfill
\includegraphics[width=0.48\textwidth]{TrainVsTestMedium}
\caption[Early stopping lower bound and training curves for overfit models]{ {\bf Left:}  We characterize the step on which early stopping occurs, as a function of the extent of overfitting.  The red line indicates a {lower bound} for early stopping that is derived in Section \ref{sec:EarlyStop}.  {\bf Right:} We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-samples.  The test loss typically follows that of a run done with unrestricted data until diverging. Note that the degree of overfitting (as compared to the infinite data limit) is significantly overestimated by $L_{\rm test} - L_{\rm train}$ (denoted by a black bar for each run). \label{fig:OverfittingandEarlyStopping}}
\end{figure}

\be
\label{eq:OverfittingPrediction}
\delta L \approx  \left( 1 +  \left(\frac{N}{N_c} \right)^{\frac{\alpha_N}{\alpha_D}} \frac{D_c}{D} \right)^{\alpha_D} - 1 
\ee
Note that at large $D$ this formula also has a series expansion in powers of $1/D$.

We estimate that the variation in the loss with different random seeds is roughly $0.02$, which means that to avoid overfitting when training to within that threshold of convergence we require 
\be
D \gtrsim (5 \times 10^3) \, N^{0.74}
\ee
\textbf{More generally, this relation shows that dataset size may grow sub-linearly in model size while avoiding overfitting.}

\section{Scaling Laws with Model Size and Training Time}
\label{sec:ScalingSizeandSteps}

\subsection{Adjustment for Training at $B_{\rm crit}(L)$}
\label{sec:OptimalBatchSize}

A simple empirical theory for the batch size dependence of training was developed in \cite{1812.06162} (see also \cite{1811.03600, DBLP:journals/corr/abs-1907-04164}).  It was argued that there is a critical batch size $B_{\rm crit}$ for training; for $B$ up to $B_{\rm crit}$  the batch size can be increased with very minimal degradation in compute-efficiency, whereas for $B > B_{\rm crit}$ increases in $B$ result in diminishing returns.  To utilize both training time and compute as effectively as possible, it is best to train with a batch size $B \approx B_{\rm crit}$.  Training at $B \gg B_{\rm crit}$  minimizes the number of training steps, while $B \ll B_{\rm crit}$ minimizes the use of compute.

More specifically, it was demonstrated that for a wide variety of neural network tasks, the number of training steps $S$ and the number of data examples processed $E = B S$ satisfy the simple relation
\be
\label{eq:TimeComputeTradeoff}
\left( \frac{S}{S_{\rm min}} -1 \right) \left( \frac{E}{E_{\rm min}} - 1 \right) = 1
\ee
when training to any fixed value of the loss $L$.  Here $S_{\rm min}$ is the minimum number of steps necessary to reach $L$, while $E_{\rm min}$ is the minimum number of data examples that must be processed. 

This relation defines the critical batch size 
\be
\label{eq:DefinitionBcrit}
B_{\rm crit}(L) \equiv \frac{E_{\rm min}}{S_{\rm min}}
\ee
which is a function of the target value of the loss.  \textbf{Training at the critical batch size makes a roughly optimal time/compute tradeoff, requiring $2 S_{\rm min}$ training steps and processing $E = 2 E_{\rm min}$ data examples.}  

In Figure \ref{fig:OptimalBatchSize} we have plotted the critical batch size and gradient noise scale as a function of training loss for two different models.  We see that $B_{\rm crit}(L)$ is  independent of model size, and only depends on the loss $L$.  The critical batch size can be fit with a power-law in the loss

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.60\textwidth]{CriticalBatchSizeVsPerf}
\caption[Critical batch size]{The critical batch size $B_{\rm crit}$ follows a power law in the loss as performance increase, and does not depend directly on the model size.  We find that the critical batch size approximately doubles for every $13\%$ decrease in loss.  $B_{\rm crit}$  is measured empirically from the data shown in Figure \ref{fig:BatchPareto}, but it is also roughly predicted by the gradient noise scale, as in \cite{1812.06162}.  \label{fig:OptimalBatchSize}}
\end{figure}

\be
B_{\rm crit}(L) \approx \frac{B_*}{L^{1/\alpha_B}}
\ee
where $B_* \approx 2 \times 10^8$ and $\alpha_B \approx 0.21$.   

We will use $B_{\rm crit}(L)$ to estimate the relation between the number of training steps $S$ while training at batch size $B = 2^{19}$ tokens and the number of training steps while training at $B \gg B_{\rm crit}$.  This is simply
\be
\label{eq:AdjustedSteps}
S_{\rm min}(S) \equiv \frac{S}{1 + B_{\rm crit}(L)/B} \qquad (\text{minimum steps, at } B \gg B_{\rm crit})
\ee
for any given target value $L$ for the loss.  This also defines a critical value of the compute needed to train to $L$ with a model of size $N$ if we were to train at $B \ll B_{\rm crit}(L)$. This is
\be
\label{eq:AdjustedCompute}
C_{\rm min}(C) \equiv \frac{C  }{1 + B/B_{\rm crit}(L) } \qquad (\text{minimum compute, at } B \ll B_{\rm crit})
\ee
where $C = 6 N BS$ estimates the (non-embedding) compute used at batch size $B$.

\subsection{Results for $L(N, S_{\rm min})$ and Performance with Model Size and Compute}

Now we will use $S_{\rm min}$ defined in Equation \eqref{eq:AdjustedSteps} to obtain a  simple and universal fit for the dependence of the loss on model size and training time in the infinite data limit.  We will fit the stable, Adam-optimized training runs using Equation \eqref{eq:FundamentalLikelihioodvsModelandSteps}, repeated here for convenience:
\be
\label{eq:FundamentalLikelihioodvsModelandSteps2}
L(N, S_{\rm min}) = \left( \frac{N_c}{N} \right)^{\alpha_N}  + \left( \frac{S_c }{S_{\rm min}} \right)^{\alpha_S}
\ee
for the loss.  We include all training steps after the warmup period of the learning rate schedule, and find  a fit to the data with the parameters:

\begin{table}[h!]
\centering
\begin{tabular}{|c| c | c | c | c| } 
 \hline
Parameter & $\alpha_N$ & $\alpha_S$ & $N_c$ & $S_c$   \\ [0.5ex] 
 \hline\hline
Value  & $0.077$ & $0.76$ & $6.5 \times 10^{13}$ & $2.1 \times 10^3$  \\ 
 \hline
\end{tabular}
\vspace{0.5em}
\caption{Fits to $L(N, S)$}
\vspace{-1em}
\end{table}

With these parameters, we obtain the learning curve fits in Figure \ref{fig:LearningCurveFitsandResiduals}.  

The data and fits can be visualized in a different and more interesting way, as shown in Figure \ref{fig:ComputevsParamsvsPerformance}.  There we study the test loss as a function of model size while fixing either the total non-embedding compute $C$ used in training, or the number of steps $S$.  For the fits we use Equation \eqref{eq:AdjustedCompute} and \eqref{eq:AdjustedSteps} along with the parameters above and Equation \eqref{eq:FundamentalLikelihioodvsModelandSteps2}.

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.47\textwidth]{PerfVsParams-ComputeBudget}\hfill
\includegraphics[width=0.47\textwidth]{PerfVsParams-StepBudget} 
\caption[Performance versus compute budget or number of parameter updates]{
When we hold either total compute or number of training steps fixed, performance follows $L(N,S)$ from Equation \eqref{eq:FundamentalLikelihioodvsModelandSteps2}.  Each value of compute budget has an associated optimal model size that maximizes performance.  Mediocre fits at small $S$ are unsurprising, as the power-law equation for the learning curves breaks down very early in training. \label{fig:ComputevsParamsvsPerformance}}
\end{figure}

\subsection{Lower Bound on Early Stopping Step}
\label{sec:EarlyStop}

The results for $L(N,S_{\rm min})$ can be used to derive a lower-bound (and rough estimate) of the step at which early stopping should occur when training is data limited.  
It is motivated by the idea that finite and infinite $D$ learning curves for a given model will be very similar until we reach $S_{\rm min} \approx S_{\rm stop}$. Thus overfitting should be proportional to the correction from simply ending  training at $S_{\rm stop}$.  This will underestimate $S_{\rm stop}$, because in reality the test loss will decrease more slowly when we have a finite $D$, and therefore we will require more training steps to reach the optimal test loss at finite $D$.  
This line of reasoning leads to the inequality
\be
\label{eq:EarlyStopInequality}
S_{\rm stop}(N,D)  \gtrsim \frac{S_c}{\left[ L(N,D) - L(N, \infty) \right]^{1 / \alpha_S}}
\ee
where $L(N, \infty)$ is the converged loss, evaluated with infinite available data.  This inequality and its comparison to the empirical data is displayed in Figure \ref{fig:OverfittingandEarlyStopping}.

\section{Optimal Allocation of the Compute Budget}
\label{sec:OptimalCompute}

\subsection{Optimal Performance and Allocations}

Let us first study the loss as a function of the optimally allocated compute from Equation \eqref{eq:AdjustedCompute}.  The result is plotted in Figure \ref{fig:ComputeEfficientAdjusted}, along with a power-law fit.  We see that as compared to the compute plot of Figure \ref{fig:BasicPowerLaws}, the new fit with $C_{\rm min}$ is somewhat improved.

\begin{figure}
\centering{}
\includegraphics[width=0.5\textwidth]{ComputeEfficientFrontierWithAdjustment}  
\caption[Comparison between empirical and adjusted compute trends]{
When adjusting performance to simulate training far below the critical batch size, we find a somewhat altered power law for $L(C_{\rm min})$ when compared with the fully empirical results.  The conspicuous lump at $10^{-5}$ PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks in the power-law fits.  It is the $L(C_{\rm min})$ trend that we expect to provide a reliable extrapolation for larger compute.
\label{fig:ComputeEfficientAdjusted}}
\end{figure}

\textbf{Given $L(C_{\rm min})$, it is natural to ask for the optimal model size $N(C_{\rm min})$ that provides the minimal loss with a given quantity of training compute.}  The optimal model size is shown in Figure \ref{fig:ComputevsPerformance}.  We observe that $N(C_{\rm min})$ can be fit very well with a power-law

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.48\textwidth]{ComputevsOptimalModelSize}\hfill
\includegraphics[width=0.48\textwidth]{ComputeEfficientSteps}
\caption[Optimal model size and serial number of steps versus compute budget]{
\textbf{Left:} Each value of the compute budget $C_{\rm min}$ has an associated optimal model size $N$.  Optimal model size grows very rapidly with $C_{\rm min}$, increasing by 5x for each 10x increase in compute.  The number of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
\textbf{Right:} The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most of the growth in data examples processed can be used for increased batch sizes.
\label{fig:ComputevsPerformance}}
\end{figure}

\be
N(C_{\rm min}) \propto (C_{\rm min})^{0.73}.
\ee
In Figure \ref{fig:SubOptimalModels}, we show the effect of training models of sub-optimal sizes (see Appendix \ref{sec:suboptimal-models}).

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.98\textwidth]{SuboptimalModels}\
\caption[Training on suboptimal models]{\textbf{Left:} Given a fixed compute budget, a particular model size is optimal, though somewhat larger or smaller models can be trained with minimal additional compute. \textbf{Right:} Models larger than the compute-efficient size require fewer steps to train, allowing for potentially faster training if sufficient additional parallelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve, after initial transient effects.  \label{fig:SubOptimalModels}}
\end{figure}

By definition $C_{\rm min} \equiv 6 N B_{\rm crit} S$, and so we can use $N(C_{\rm min})$ to extract further results.  In particular, since prior fits show $B \propto L^{-4.8}$ and $L \propto C_{\rm min}^{-0.05}$, we can conclude that $B_{\rm crit} \propto C_{\rm min}^{0.24}$.  \textbf{This leads us to conclude that the optimal number of steps will only grow very slowly with compute}, as
\be
S_{\rm min} \propto (C_{\rm min})^{0.03},
\ee
matching the empirical results in Figure \ref{fig:ComputevsPerformance}.  In fact the measured exponent is sufficiently small that our results may even be consistent with an exponent of zero.  

\textbf{Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we should predominantly increase the model size $N$, while simultaneously scaling up the batch size  via $B \propto B_{\rm crit}$ with negligible increase in the number of serial steps.}

\subsection{Predictions from $L(N, S_{\rm min})$}

For the loss as a function of training compute, we predict that
\be
L(C_{\rm min}) = \left( \frac{C_c^{\rm min}}{C_{\rm min}} \right)^{\alpha_C^{\rm min}}
\ee
where
\be
\alpha_C^{\rm min} \equiv \frac{1}{1/\alpha_S + 1/\alpha_B + 1/\alpha_N} \approx 0.054
\ee
in excellent agreement with the exponent of Figure \ref{fig:ComputeEfficientAdjusted}.  We also predict that
\be
N(C_{\rm min}) \propto (C_{\rm min})^{\alpha_C^{\rm min} / \alpha_N} \approx  (C_{\rm min})^{0.71}
\ee
which also matches the scaling of Figure \ref{fig:ComputevsPerformance} to within a few percent.

\subsection{Contradictions and a Conjecture}

We observe no signs of deviation from straight power-law trends at large values of compute, data, or model size. Our trends must eventually level off, though, since natural language has non-zero entropy. 

Indeed, the trends for compute-efficient training described in this section already contain an apparent contradiction. At scales several orders of magnitude above those documented here, the performance predicted by the $L(C_{\rm min})$ scaling law decreases below what should be possible given the slow growth in training data with compute.  This implies that our scaling laws must break down before this point, but we conjecture that the intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language models reach maximal performance.

Since the amount of data used by compute-efficient training grows  slowly with the compute budget, the performance predicted by $L(C_{\rm min})$ eventually hits a lower bound set by the $L(D)$ power law (see Figure \ref{fig:Contradiction}).  Let us work this out in more detail.

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.8\textwidth]{Contradiction}
\caption[Contradiction between compute and data trends]{Far beyond the model sizes we study empirically, we find a contradiction between our equations for $L(C_{\rm min})$ and $L(D)$ due to the slow growth of data needed for compute-efficient training.  The intersection marks the point before which we expect our predictions to break down.  The location of this point is highly sensitive to the precise exponents from our power-law fits. \label{fig:Contradiction}}
\end{figure}

To keep overfitting under control, the results of Section \ref{sec:ChartingOverfitting} imply that we should scale the dataset size as
\be
\label{eq:DataGrowthOverfitting}
D \propto N^{0.74} \propto C_{\rm min}^{0.54}
\ee
where we have used the compute-efficient $N(C_{\rm min})$ from Figure \ref{fig:ComputevsPerformance}.

Let us compare this to the data requirements of compute-efficient training.  If we train at the critical batch size (i.e. $C=2C_{\rm min}$) and never re-use data during training, we find that data usage grows with compute as
\be
D (C_{\rm min}) = \frac{2C_{\rm min}}{6 N(C_{\rm min})} \approx \left( 4 \times 10^{10}  \ {\rm tokens} \right) (C_{\rm min} / \mathrm{PF}{\text -}\mathrm{Day}  )^{0.26} 
\ee
This is the maximum rate at which the dataset size can productively grow with compute, since it means that we are only training for a single epoch.  But it grows the dataset much more slowly than in Equation \eqref{eq:DataGrowthOverfitting}.  It appears to imply that compute-efficient training will eventually run into a problem with overfitting, even if the training process never re-uses any data!


 
According to Figure \ref{fig:BasicPowerLaws}, we expect that when we are bottlenecked by the dataset size (ie by overfitting), the loss should scale as $L(D) \propto D^{-0.095}$.  This implies that the loss would scale with compute as $L(D(C_{\rm min})) \propto C_{\rm min}^{-0.03}$ once we are data-limited.  Once again, we have a contradiction, as this will eventually intersect with our prediction for $L(C_{\rm min})$ from Figure \ref{fig:ComputeEfficientAdjusted}, where we found a scaling $L(C_{\rm min}) \propto C_{\rm min}^{-0.050}$.  

The intersection point of $L(D(C_{\rm min}))$ and $L(C_{\rm min})$ occurs at
\be
C^* \sim 10^4~\mathrm{PF}{\text -}\mathrm{Days}
\quad
N^* \sim 10^{12}~\text{parameters},
\quad
D^* \sim 10^{12}~\text{tokens},
\quad
L^* \sim 1.7~\text{nats/token}
\label{eq:SpecialPoint}
\ee
though the numerical values are highly uncertain, varying by an order or magnitude in either direction depending on the precise values of the exponents from the power-law fits.  The most obvious interpretation is that our scaling laws break down at or before we reach this point, which is still many orders of magnitude away in both compute and model size.  

One might also conjecture that this intersection point has a deeper meaning.  If we cannot increase the model size beyond $N^*$ without qualitatively different data requirements, perhaps this means that once we reach $C_{\rm min}^*$ and $N^*$, we have extracted all of the reliable information available in natural language data.  In this interpretation, $L^*$ would provide a rough estimate for the entropy-per-token of natural language.  In this scenario, we would expect the loss trend to level off at or before $L^*$.

\section{Discussion}


We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter count $N$, dataset size $D$, and optimized training computation $C_{\rm min}$, as encapsulated in Equations \eqref{eq:FundamentalLikelihioodvsModelandDataSize} and \eqref{eq:FundamentalLikelihioodvsModelandSteps}.  Conversely, we find very weak dependence on many architectural and optimization hyperparameters.  Since scalings with $N, D, C_{\rm min}$ are power-laws, there are diminishing returns with increasing scale.

In the domain of natural language, it will be important to investigate whether continued improvement on the loss translates into improvement on relevant language tasks.  Smooth quantitative change can mask major qualitative improvements: ``more is different''.  The smooth improvements in language model loss may hide seemingly qualitative changes in capability.

Our results strongly suggest that larger models will continue to perform better, and will also be much more sample efficient than has been previously appreciated.  Big models may be more important than big data.  In this context, further investigation into model parallelism is warranted.

\newpage
\appendix
\appendixpage
\addappheadtotoc 


\section{Summary of Power Laws}

For easier reference, we provide a summary below of the key trends described throughout the paper.

\begin{table}[h!]
\centering
\vspace{-0.5em}
\begin{tabular}{|c|c|c|c|l|}
\hline 
\textbf{Parameters}  & \textbf{Data}  & \textbf{Compute}  & \textbf{Batch Size}  & \textbf{Equation}\tabularnewline
\hline 
\hline 
$N$  & $\infty$  & \multicolumn{1}{c|}{$\infty$ } & Fixed  & $L\left(N\right)=\left(N_{{\rm c}}/N\right)^{\alpha_{N}}$\tabularnewline
\hline 
$\infty$  & $D$  & \multicolumn{1}{c|}{Early Stop } & Fixed  & $L\left(D\right)=\left(D_{{\rm c}}/D\right)^{\alpha_{D}}$\tabularnewline
\hline 
Optimal  & $\infty$  & $C$  & Fixed  & $L\left(C\right)=\left(C_{{\rm c}}/C\right)^{\alpha_{C}}$ (naive)\tabularnewline
\hline 
$N_{{\rm opt}}$ & $D_{{\rm opt}}$ & $C_{{\rm min}}$  & $B\ll B_{{\rm crit}}$  & $L\left(C_{{\rm min}}\right)=\left(C_{{\rm c}}^{{\rm min}}/C_{{\rm min}}\right)^{\alpha_{C}^{{\rm min}}}$\tabularnewline
\hline 
$N$  & $D$  & \multicolumn{1}{c|}{Early Stop } & Fixed  & $L\left(N,D\right)=\left[\left(\frac{N_{{\rm c}}}{N}\right)^{\frac{\alpha_{N}}{\alpha_{D}}}+\frac{D_{c}}{D}\right]^{\alpha_{D}}$\tabularnewline
\hline 
$N$  & $\infty$  & $S$ steps & $B$  & $L\left(N,S\right)=\left(\frac{N_{{\rm c}}}{N}\right)^{\alpha_{N}}+\left(\frac{S_{{\rm c}}}{S_{{\rm min}}\left(S,B\right)}\right)^{\alpha_{S}}$\tabularnewline
\hline 
\end{tabular}
\vspace{0.5em}
\caption[Key trend equations]{}
\vspace{-1em}
\end{table}

The empirical fitted values for these trends are:


\begin{table}[h!]
\centering
\vspace{-0.5em}
\begin{tabular}{|l|l|}
\hline 
\textbf{Power Law}  & \textbf{Scale (tokenization-dependent)}\tabularnewline
\hline 
\hline 
$\alpha_{N}=0.076$  & $N_{{\rm c}}=8.8\times10^{13}$ params (non-embed)\tabularnewline
\hline 
$\alpha_{D}=0.095$  & $D_{{\rm c}}=5.4\times10^{13}$ tokens\tabularnewline
\hline 
$\alpha_{C}=0.057$  & $C_{{\rm c}}=1.6\times10^{7}$ PF-days\tabularnewline
\hline 
$\alpha_{C}^{{\rm min}}=0.050$  & $C_{{\rm c}}^{{\rm min}}=3.1\times10^{8}$ PF-days\tabularnewline
\hline 
$\alpha_{B}=0.21$  & $B_{\ast}=2.1\times10^{8}$ tokens\tabularnewline
\hline 
$\alpha_{S}=0.76$  & $S_{{\rm c}}=2.1\times10^{3}$ steps\tabularnewline
\hline 
\end{tabular}
\vspace{0.5em}
\caption[Key parameters to trend fits]{}
\vspace{-1em}
\end{table}

The optimal parameters for compute efficient training are given by:

\begin{table}[h!]
\centering
\vspace{-0.5em}
\begin{tabular}{|l|l|l|}
\hline 
\textbf{Compute-Efficient Value} & \textbf{Power Law} & \textbf{Scale}\tabularnewline
\hline 
\hline 
$N_{{\rm opt}}=N_{e}\cdot C_{{\rm min}}^{p_{N}}$ & $p_{N}=0.73$ & $N_{e}=1.3\cdot10^{9}$ params\tabularnewline
\hline 
$B \ll B_{{\rm crit}}=\frac{B_{\ast}}{L^{1/\alpha_{B}}}=B_{e}C_{{\rm min}}^{p_{B}}$ & $p_{B}=0.24$ & $B_{e}=2.0\cdot10^{6}$ tokens\tabularnewline
\hline 
$S_{{\rm min}}=S_{e}\cdot C_{{\rm min}}^{p_{S}}$ (lower bound) & $p_{S}=0.03$ & $S_{e}=5.4\cdot10^{3}$ steps\tabularnewline
\hline 
$D_{{\rm opt}}=D_{e}\cdot C_{{\rm min}}^{p_{D}}$ (1 epoch) & $p_{D}=0.27$ & $D_{e}=2\cdot10^{10}$ tokens\tabularnewline
\hline 
\end{tabular}
\vspace{0.5em}
\caption[Trends for compute-efficient training]{}
\vspace{-1em}
\end{table}





\section{Empirical Model of Compute-Efficient Frontier}
\label{app:ComputeEfficientTraining}

\subsection{Efficient Training}

\begin{equation}
L\left(N_{{\rm eff}}\left(C\right),C\right)=\left(1+\frac{\alpha_{N}}{\alpha_{S}}\right)L\left(N_{{\rm eff}},\infty\right),
\end{equation}
which implies that for compute-efficient training, we should train to a \textbf{fixed percentage} $\frac{\alpha_{N}}{\alpha_{S}}\approx10\%$ above the converged loss.
Next, let's determine how the optimal loss depends on the compute budget. Eliminating $N$ yields a power-law dependence of performance on compute:
\begin{align}
L\left(C\right) & =\left(\frac{C_{c}}{C}\right)^{\alpha_{C}}
\end{align}
where we defined
\begin{align}
\alpha_{C} & =1/\left(1/\alpha_{S}+1/\alpha_{B}+1/\alpha_{N}\right)\approx0.052\\
C_{c} & =6N_{c}B_{\ast}S_{c}\left(1+\frac{\alpha_{N}}{\alpha_{S}}\right)^{1/\alpha_{S}+1/\alpha_{N}}\left(\frac{\alpha_{S}}{\alpha_{N}}\right)^{1/\alpha_{S}}.
\end{align}
Similarly, we can eliminate $L$ to find $N\left(C\right)$:
\begin{align}
\frac{N\left(C\right)}{N_{c}}=\left(\frac{C}{C_{c}}\right)^{\alpha_{C}/\alpha_{N}}\left(1+\frac{\alpha_{N}}{\alpha_{S}}\right)^{1/\alpha_{N}}
\end{align}
and
\begin{align}
S\left(C\right) & =\frac{C_{c}}{6N_{c}B_{\ast}}\left(1+\frac{\alpha_{N}}{\alpha_{S}}\right)^{-1/\alpha_{N}}\left(\frac{C}{C_{c}}\right)^{\alpha_{C}/\alpha_{S}}
\end{align}

\subsection{Comparison to Inefficient}
Typically, researchers train models until they appear to be close to convergence. In this section, we compare the efficient training procedure described above to this more typical setup.
We define a the convergence factor $f$ as the percent deviation from the converged loss:
\begin{equation}
L\left(N,C\right)=\left(1+f\right)L\left(N,\infty\right).
\end{equation}
For compute-efficient training we have $f=\alpha_{N}/\alpha_{S}\approx10\%$ from the previous section, but researchers typically use a much smaller value. Here, we choose $f^{\prime}=2\%$ as an estimate.
For a fixed value of the loss, we predict:
\begin{align}
\frac{N_{f}}{N_{f^{\prime}}} & =\left(\frac{1+f}{1+f^{\prime}}\right)^{1/\alpha_{N}}\approx2.7\\
\frac{S_{f}}{S_{f^{\prime}}} & =\left(\frac{1+\frac{1}{f}}{1+\frac{1}{f^{\prime}}}\right)^{1/\alpha_{S}}\approx0.13\\
\frac{C_{f}}{C_{f^{\prime}}} & =\frac{N_{f}}{N_{f^{\prime}}}\frac{S_{f}}{S_{f^{\prime}}}\approx0.35
\end{align}
So that compute-efficient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65\% less compute to reach the same loss.

\subsection{Suboptimal Model Sizes}
\label{sec:suboptimal-models}
We can solve A.1 to find an expression for the amount of compute needed to reach a given value of the loss $L$ with a model of size $N$:
\be
C\left(N,L\right)=\left(6B_{\ast}S_{c}\frac{N}{L^{1/\alpha_{B}}}\right)\left(L-\left(\frac{N_{c}}{N}\right)^{\alpha_{N}}\right)^{-1/\alpha_{S}}.
\ee
Using A.6 and A.9, we can eliminate $L$ in favor of $N_{{\rm eff}}\left(L\right)$, the model size which reaches $L$ most efficiently. From there, we find an expression for the excess compute needed as a consequence of using a suboptimal model size: 
\be
\frac{C\left(N,N_{{\rm eff}}\right)}{C\left(N_{{\rm eff}},N_{{\rm eff}}\right)}=\frac{N}{N_{{\rm eff}}}\left[1+\frac{\alpha_{S}}{\alpha_{N}}\left(1-\left(\frac{N_{{\rm eff}}}{N}\right)^{\alpha_{N}}\right)\right]^{-1/\alpha_{S}}.
\ee
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a 20\% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism and faster training if sufficient harware is available (see Figure Y):
\be
\frac{S\left(N,N_{{\rm eff}}\right)}{S\left(N_{{\rm eff}},N_{{\rm eff}}\right)}=\left[1+\frac{\alpha_{S}}{\alpha_{N}}\left(1-\left(\frac{N_{{\rm eff}}}{N}\right)^{\alpha_{N}}\right)\right]^{-1/\alpha_{S}}.
\ee
A 2.2x larger model requires 45\% fewer steps at a cost of 20\% more training compute.

\section{Supplemental  Figures}

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.48\textwidth]{BatchParetoFronts3M} 
\includegraphics[width=0.48\textwidth]{BatchParetoFronts85M}
 \caption[Batch size scans]{These figures demonstrate fits to Equation \eqref{eq:TimeComputeTradeoff} for a large number of values of the loss $L$, and for two different Transformer model sizes.  These fits were used to measure $B_{\rm crit}(L)$ for Figure \ref{fig:OptimalBatchSize}.  \label{fig:BatchPareto}}
\end{figure}

\begin{figure}
\noindent \centering{}
\includegraphics[width=0.49\textwidth]{SampleEfficiencystep_min}
\includegraphics[width=0.49\textwidth]{SampleEfficiencyexamples_min}
\caption[Another look at sample efficiency]{The number of minimum serial steps needed to reach any fixed value of the test loss decreases precipitously with model size.  Sample efficiency (show here for training far below the critical batch size) improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model to a very large one.   \label{fig:SampleEfficiency}}
\end{figure} 

\begin{figure}
\noindent \centering{} \includegraphics[width=0.50\textwidth]{ContextPowerLaw} 
\includegraphics[width=0.48\textwidth]{SingleRunTokenTraining}  \caption[Power-law dependence of performance on position in context]{ This figure provides information about the performance per token as a function of model size and training time.  {\bf Left:} Loss per token as a function of its position $T$ in the 1024-token context.  Loss scales predictably as a power-law in $T$.  {\bf Right: } Test loss per token as a function of training step.  \label{fig:MoreTokenAnalysis}}
\end{figure}



\subsection{Context Dependence}
\label{sec:ContextDependence}

\begin{figure}
\noindent \centering{}  \includegraphics[width=0.54\textwidth]{PerformancevsModelSizevsContext}  \caption[Performance at different context positions versus model size]{In addition to the averaged loss, individual tokens within the 1024-token context also improve smoothly as model size increases.  Training runs with shorter context $n_{\rm ctx} = 8$ (dashed lines) perform better on early tokens, since they can allocate all of their capacity to them.  \label{fig:PerformancevsModelSizevsContext}}
\end{figure}

Fixing model size, it appears that the loss scales as a power-law as a function of position $T$ in the context, see Figure \ref{fig:MoreTokenAnalysis}.  It provides some suggestion for the potential benefits (or lack thereof) from training on larger contexts.  Not only do larger models converge to better performance at $T=1024$, but they also improve more quickly at early tokens, suggesting that larger models are more efficient at detecting patterns with less contextual information.  In the right-hand plot we show how per-token performance varies for a fixed model as a function of the training step.  The model begins by learning short-range information, and only learns longer-range correlations later in training.

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.46\textwidth]{VariousLearningSchedules} 
\includegraphics[width=0.46\textwidth]{LearningRateSchedulesvsPerformance} 
 \caption[Learning rate schedule scan]{
 We test a variety of learning rate schedules including cosine decay, linear decay, as well as other faster/slower decays schedules on a 3 million parameter model, shown on the left.
 For these experiments we do not decay to zero, since we find that this tends to give a fixed improvement close to the end of training.
 We find that, as long as the learning rate is not too small and does not decay too quickly, performance does not depend strongly on learning rate.
 Run-to-run variation is at the level of ~0.05 in the loss, so averaging multiple runs is necessary to validate performance changes smaller than this level.
 \label{fig:LearningRateSchedules} }
\end{figure}

\subsection{Learning Rate Schedules and Error Analysis}
\label{app:OptimizationDetailsandErrorAnalysis}

We found that larger models require a smaller learning rate to prevent divergence, while smaller models can tolerate a larger learning rate.  To implement this, the following rule of thumb was used for most runs:
\begin{equation}
\mathrm{LR}(N) \approx 0.003239 + -0.0001395  \log(N)
\end{equation}

\begin{figure}
\noindent \centering{} 
\includegraphics[width=0.40\textwidth]{PoorLogFit}
\caption[Comparison of Power-Law and Logarithmic Fits]{The trend for performance as a function of parameter count, $L(N)$, is fit better by a power law than by other functions such as a logarithm at a qualitative level.  \label{fig:PoorLogFit}}
\end{figure}

\listoffigures
\listoftables

\bibliographystyle{halpha}
%\nocite{*}  % Enable this to show all references in the bib file
\bibliography{bibliography}

\end{document}








