# paper-is-all-you-need

论文就是你所需要的。

## 论文

|论文|年份|论文单位|笔记地址|
|:-:|:-:|:-:|:-:|
|[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)|2020|OpenAI|[./papers/00001-scaling-laws.pdf](./papers/00001-scaling-laws.pdf)|
|[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)|2020|Microsoft|[./papers/00002-ZeRO.pdf](./papers/00002-ZeRO.pdf)|
|[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395)|2024|Tsinghua University|[./papers/00003-MiniCPM.pdf](./papers/00003-MiniCPM.pdf)|
|[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)|2024|Microsoft|[./papers/00004-Phi-3.pdf](./papers/00004-Phi-3.pdf)|
|[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/abs/2406.12793)|2024|Tsinghua University|[./papers/00005-ChatGLM.pdf](./papers/00005-ChatGLM.pdf)|
|[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)|2024|Meta|[./papers/00006-Llama3.pdf](./papers/00006-Llama3.pdf)
|[AlphaMath Almost Zero: process Supervision without process](https://arxiv.org/abs/2405.03553)|2024|Alibaba Group|[./papers/00007-AlphaMath.pdf](./papers/00007-AlphaMath.pdf)|
|[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)|2022|Google|[./papers/00008-CoT.pdf](./papers/00008-CoT.pdf)|
|[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)|2021|Zhuiyi Technology|[./papers/00009-RoPE.pdf](./papers/00009-RoPE.pdf)|
|[Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)|2024|Alibaba Group|[./papers/00010-Qwen2.5-Coder.pdf](./papers/00010-Qwen2.5-Coder.pdf)|
|[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/abs/2404.04167)|2024|University of Waterloo|[./papers/00011-Chinese-Tiny-LLM.pdf](./papers/00011-Chinese-Tiny-LLM.pdf)|
|[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)|2022|NVIDIA|[./papers/00012-selective-activation-recomputation.pdf](./papers/00012-selective-activation-recomputation.pdf)|
|[DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://arxiv.org/abs/2308.01320)|2023|Microsoft|[./papers/00013-DeepSpeed-Chat.pdf](./papers/00013-DeepSpeed-Chat.pdf)|
|[ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks](https://arxiv.org/abs/2312.08583)|2023|Microsoft|[./papers/00014-ZeroQuant(4+2).pdf](./papers/00014-ZeroQuant(4+2).pdf)|
|[DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale](https://arxiv.org/abs/2207.00032)|2022|Microsoft|[./papers/00015-DeepSpeed-Inference.pdf](./papers/00015-DeepSpeed-Inference.pdf)|
|[DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://arxiv.org/abs/2309.14509)|2023|Microsoft|[./papers/00016-DeepSpeed-Ulysses.pdf](./papers/00016-DeepSpeed-Ulysses.pdf)|

## 工具

1. Overleaf: https://www.overleaf.com/

## 论文集

1. [huggingface daily papers](https://huggingface.co/papers)
