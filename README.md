# paper-is-all-you-need

论文就是你所需要的。

## 论文

|论文|年份|论文单位|笔记地址|
|:-:|:-:|:-:|:-:|
|[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)|2020|OpenAI|[./papers/00001-scaling-laws.pdf](./papers/00001-scaling-laws.pdf)|
|[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)|2020|Microsoft|[./papers/00002-ZeRO.pdf](./papers/00002-ZeRO.pdf)|
|[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395)|2024|Tsinghua University|[./papers/00003-MiniCPM.pdf](./papers/00003-MiniCPM.pdf)|
|[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)|2024|Microsoft|[./papers/00004-Phi-3.pdf](./papers/00004-Phi-3.pdf)|
|[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/abs/2406.12793)|2024|Tsinghua University|[./papers/00005-ChatGLM.pdf](./papers/00005-ChatGLM.pdf)|
|[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)|2024|Meta|[./papers/00006-Llama3.pdf](./papers/00006-Llama3.pdf)
|[AlphaMath Almost Zero: process Supervision without process](https://arxiv.org/abs/2405.03553)|2024|Alibaba Group|[./papers/00007-AlphaMath.pdf](./papers/00007-AlphaMath.pdf)|
|[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)|2022|Google|[./papers/00008-CoT.pdf](./papers/00008-CoT.pdf)|
|[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)|2021|Zhuiyi Technology|[./papers/00009-RoPE.pdf](./papers/00009-RoPE.pdf)|
|[Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)|2024|Alibaba Group|[./papers/00010-Qwen2.5-Coder.pdf](./papers/00010-Qwen2.5-Coder.pdf)|
|[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/abs/2404.04167)|2024|University of Waterloo|[./papers/00011-Chinese-Tiny-LLM.pdf](./papers/00011-Chinese-Tiny-LLM.pdf)|
|[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)|2022|NVIDIA|[./papers/00012-selective-activation-recomputation.pdf](./papers/00012-selective-activation-recomputation.pdf)|

## 工具

1. Overleaf: https://www.overleaf.com/

## 论文集

1. [huggingface daily papers](https://huggingface.co/papers)
